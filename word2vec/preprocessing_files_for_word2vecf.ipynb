{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTEBOOK DESCRIPTION:\n",
    "\n",
    "This notebook creates the input files needed to run the word2vecf source code.\n",
    "\n",
    "- training_data: Provide all pairs of channel-users present in the `comments_dataset`. For example, if user $user_i$ commented on $channel_i$ 3 times, the pair $(channel_i, user_i)$ would appear 3 times in the training data,\n",
    "- word_vocabulary: Text file. For each channels, provide the number of comments commented on this channel,\n",
    "- context_vocabulary: Text file. For each user, provide the number of commented has done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "scriptpath = \"../\"\n",
    "sys.path.append(os.path.abspath(scriptpath))\n",
    "from helpers.helpers_channels_more_10k import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select a maximum number of user because of the requirement from word2vecf source code\n",
    "NB_SELECTED_USERS = 30000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load and filter the data\n",
    "Select the NB_SELECTED_USERS having the most comments on different channels\n",
    "\n",
    "RETURN:\n",
    "    - Sparse matrix containing only the selected users\n",
    "    - The indices of the set of selected_users\n",
    "'''\n",
    "def filter_users_by_number_of_comments_in_different_channels():\n",
    "    # Load the channel tuple sparse matrix\n",
    "    S = scipy.sparse.load_npz('/dlabdata1/youtube_large/jouven/final_sparse_matrix/channels_more_10k/word2vecf/sparse_matrix_for_word2vec.npz')\n",
    "    comments_on_different_channels = S.getnnz(axis = 0)\n",
    "    selected_users = comments_on_different_channels.argsort()[len(comments_on_different_channels) - NB_SELECTED_USERS:]\n",
    "    return S[:, selected_users], selected_users\n",
    "\n",
    "'''\n",
    "Load and filter the data\n",
    "Select the NB_SELECTED_USERS having the most comments\n",
    "\n",
    "RETURN:\n",
    "    - Sparse matrix containing only the selected users\n",
    "    - The indices of the set of selected_users\n",
    "'''\n",
    "def filter_users_by_number_of_comments():\n",
    "    # Load the channel tuple sparse matrix\n",
    "    S = scipy.sparse.load_npz('/dlabdata1/youtube_large/jouven/final_sparse_matrix/channels_more_10k/word2vecf/sparse_matrix_for_word2vec.npz')\n",
    "    S = S.tocsc()\n",
    "    users_occurences = np.array(S.sum(axis = 0).flatten().tolist()[0])  # sum the columns\n",
    "    selected_users = users_occurences.argsort()[len(users_occurences) - 2*NB_SELECTED_USERS:NB_SELECTED_USERS]\n",
    "    return S[:, selected_users], selected_users\n",
    "\n",
    "'''\n",
    "Load and filter the data\n",
    "Select the NB_SELECTED_USERS having the most comments and having more than THRESHOLD comments\n",
    "on different channels\n",
    "\n",
    "RETURN:\n",
    "    - Sparse matrix containing only the selected users\n",
    "    - The indices of the set of selected_users\n",
    "'''\n",
    "def filter_users_by_number_comments_and_number_of_comments_in_different_channels():\n",
    "    \n",
    "    THRESHOLD = 20\n",
    "    # Load the channel tuple sparse matrix\n",
    "    S = scipy.sparse.load_npz('/dlabdata1/youtube_large/jouven/final_sparse_matrix/channels_more_10k/word2vecf/sparse_matrix_for_word2vec.npz')\n",
    "    \n",
    "    users_occurences = S.getnnz(axis = 0)\n",
    "    selected_users = set(np.where(users_occurences >= THRESHOLD)[0])\n",
    "    \n",
    "    users_occurences = np.array(S.sum(axis = 0).flatten().tolist()[0]).argsort()  # sum the columns and sort\n",
    "    \n",
    "    final_user_selection = []\n",
    "    for val in reversed(users_occurences):\n",
    "        if val in selected_users:\n",
    "            final_user_selection.append(val)\n",
    "    \n",
    "    #selected_users = list(users_occurences.intersection(selected_users))[len(selected_users) - NB_SELECTED_USERS:]\n",
    "    \n",
    "    S = S.tocsc()\n",
    "    S = S.T\n",
    "    S = S[selected_users]\n",
    "    S = S.T\n",
    "    return S, selected_users\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create a text file named word_vocabulary, the word here corresponds to each channel\n",
    "Each line corresponds to a (channel, # occurences) pair and written as\n",
    "\"channel_index #occurences\" in the file\n",
    "'''\n",
    "def create_word_vocabulary(S, SAVING_PATH):\n",
    "    word_pair = \"\"\n",
    "    occurences_tab = np.array(S.sum(axis = 1).flatten().tolist()[0])  # sum the rows\n",
    "    occurences_tab_ind = occurences_tab.argsort()\n",
    "    for ind in occurences_tab_ind:\n",
    "        #ind represents the channel index\n",
    "        word_pair += str(ind) + \" \" + str(occurences_tab[ind]) + \"\\n\"\n",
    "    print('Writing into word_vocabulary...')\n",
    "    f = open(SAVING_PATH, \"w\")\n",
    "    f.write(word_pair)\n",
    "    f.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create a text file named context_vocabulary, the context here is the uset\n",
    "Each line corresponds to a (user, # occurences) pair and written as\n",
    "\"user_index #occurences\" in the file\n",
    "'''\n",
    "def create_context_vocabulary(S, SAVING_PATH):\n",
    "    \n",
    "    context_pair = \"\"\n",
    "    # Temporary variable for speed purpose\n",
    "    context_pair_temp = \"\"\n",
    "    \n",
    "    occurences_tab = np.array(S.sum(axis = 0).flatten().tolist()[0])  # sum the columns\n",
    "    # Select the 30M users that commented the most\n",
    "    occurences_tab_ind = occurences_tab.argsort()\n",
    "    i = 0\n",
    "    for ind in occurences_tab_ind:\n",
    "        context_pair_temp += str(ind) + \" \" + str(occurences_tab[ind]) + \"\\n\"\n",
    "        if i % 10000 == 0 and i > 0:\n",
    "            context_pair += context_pair_temp\n",
    "            context_pair_temp = \"\"\n",
    "        i += 1\n",
    "        \n",
    "    occurences_tab = []\n",
    "    \n",
    "    context_pair += context_pair_temp\n",
    "    context_pair_temp = \"\"\n",
    "    print('Writing into context_vocabulary...')\n",
    "    f = open(SAVING_PATH, \"w\")\n",
    "    f.write(context_pair)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "From the already exsiting text file of channel-user pairs, we select only lines containing\n",
    "users in selected_users to create a new training_file for word2vecf\n",
    "PARAMETERS:\n",
    "    - The set of selected users\n",
    "'''\n",
    "def create_channels_users_pairs(selected_users):\n",
    "    # Make selected_users \n",
    "    selected_users = set(selected_users)\n",
    "    \n",
    "    f2 = open(\"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/channels_more_10k/size_200_sub_0043_neg_35_threshold_20/training_data\", \"w\")\n",
    "    f2.close()\n",
    "    \n",
    "    f = open(\"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/channels_more_10k/training_data\", 'r') \n",
    "    i = 0\n",
    "    new_training_data = ''\n",
    "    new_training_data_temp = ''\n",
    "    while True: \n",
    "        # Get next line from file \n",
    "        line = f.readline() \n",
    "\n",
    "        # if line is empty \n",
    "        # end of file is reached \n",
    "        if not line: \n",
    "            break\n",
    "            \n",
    "        # Select the channels-users pairs where the user is in the set of selected users\n",
    "        if int(line.split(' ')[1].split('\\n')[0]) in selected_users:\n",
    "            new_training_data_temp += line\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "        if i % 100000 == 0:\n",
    "            new_training_data += new_training_data_temp\n",
    "            new_training_data_temp = \"\"\n",
    "            \n",
    "        if i % 100000000 == 0:\n",
    "            f2 = open(\"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/channels_more_10k/size_200_sub_0043_neg_35_threshold_20/training_data\", \"a\")\n",
    "            f2.write(new_training_data)\n",
    "            f2.close()\n",
    "            new_training_data = \"\"\n",
    "            new_training_data_temp = \"\"\n",
    "            print(str(100000000) + ' lines have been processed')\n",
    "            \n",
    "    f.close() \n",
    "    f2 = open(\"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/channels_more_10k/size_200_sub_0043_neg_35_threshold_20/training_data\", \"a\")\n",
    "    f2.write(new_training_data)\n",
    "    f2.close()\n",
    "    new_training_data = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S, selected_users = filter_users_by_number_comments_and_number_of_comments_in_different_channels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing into context_vocabulary...\n"
     ]
    }
   ],
   "source": [
    "create_context_vocabulary(S, \"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/channels_more_10k/size_200_sub_0043_neg_35_threshold_20/context_vocabulary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing into word_vocabulary...\n"
     ]
    }
   ],
   "source": [
    "create_word_vocabulary(S, \"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/channels_more_10k/size_200_sub_0043_neg_35_threshold_20/word_vocabulary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n",
      "100000000 lines have been processed\n"
     ]
    }
   ],
   "source": [
    "S = []\n",
    "create_channels_users_pairs(selected_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
