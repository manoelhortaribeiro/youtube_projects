{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "   def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.files = os.listdir(root) # take all files in the root directory\n",
    "   def __len__(self):\n",
    "        return len(self.files)\n",
    "   def __getitem__(self, idx):\n",
    "        sample, label = torch.load(os.path.join(self.root, self.files[idx])) # load the features of this sample\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 8}\n",
    "max_epochs = 100\n",
    "\n",
    "# Datasets\n",
    "partition = # IDs\n",
    "labels = # Labels\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(partition['train'], labels)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(partition['validation'], labels)\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n",
    "\n",
    "\n",
    "vocab_size = 20000\n",
    "word2vec = Word2Vec(vocab_size=vocab_size, embedding_size=300)\n",
    "sgns = SGNS(embedding=word2vec, vocab_size=vocab_size, n_negs=20)\n",
    "optim = Adam(sgns.parameters())\n",
    "for batch, (iword, owords) in enumerate(dataloader):\n",
    "    iword, owords = iword.to(device), owords.to(device)\n",
    "    loss = sgns(iword, owords)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    \n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    # Training\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "        # Model computations\n",
    "        [...]\n",
    "\n",
    "    # Validation\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for local_batch, local_labels in validation_generator:\n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "\n",
    "def get_input_tensor(tensor):\n",
    "    '''Transform 1D tensor of word indexes to one-hot encoded 2D tensor'''\n",
    "    size = [*tensor.shape][0]\n",
    "    inp = torch.zeros(size, vocab_size).scatter_(1, tensor.unsqueeze(1), 1.)\n",
    "    return Variable(inp).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dims = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initrange = 0.5 / embedding_dims\n",
    "W1 = Variable(torch.randn(vocab_size, embedding_dims, device=device).uniform_(-initrange, initrange).float(), requires_grad=True) # shape V*H\n",
    "W2 = Variable(torch.randn(embedding_dims, vocab_size, device=device).uniform_(-initrange, initrange).float(), requires_grad=True) #shape H*V\n",
    "print(f'W1 shape is: {W1.shape}, W2 shape is: {W2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "learning_rate = 2e-1\n",
    "lr_decay = 0.99\n",
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epo in range(num_epochs):\n",
    "    for x,y in zip(DataLoader(train_emb.Input.values, batch_size=train_emb.shape[0]), DataLoader(train_emb.Output.values, batch_size=train_emb.shape[0])):\n",
    "        \n",
    "        # one-hot encode input tensor\n",
    "        input_tensor = get_input_tensor(x) #shape N*V\n",
    "     \n",
    "        # simple NN architecture\n",
    "        h = input_tensor.mm(W1) # shape 1*H\n",
    "        y_pred = h.mm(W2) # shape 1*V\n",
    "        \n",
    "        # define loss func\n",
    "        loss_f = torch.nn.CrossEntropyLoss() # see details: https://pytorch.org/docs/stable/nn.html\n",
    "        \n",
    "        #compute loss\n",
    "        loss = loss_f(y_pred, y)\n",
    "        \n",
    "        # bakpropagation step\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights using gradient descent. For this step we just want to mutate\n",
    "        # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "        # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "        # to prevent PyTorch from building a computational graph for the updates\n",
    "        with torch.no_grad():\n",
    "            # SGD optimization is implemented in PyTorch, but it's very easy to implement manually providing better understanding of process\n",
    "            W1 -= learning_rate*W1.grad.data\n",
    "            W2 -= learning_rate*W2.grad.data\n",
    "            # zero gradients for next step\n",
    "            W1.grad.data.zero_()\n",
    "            W1.grad.data.zero_()\n",
    "    if epo%10 == 0:\n",
    "        learning_rate *= lr_decay\n",
    "    loss_hist.append(loss)\n",
    "    if epo%50 == 0:\n",
    "        print(f'Epoch {epo}, loss = {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
