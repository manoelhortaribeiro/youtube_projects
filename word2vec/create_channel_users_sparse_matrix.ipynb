{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTEBOOK DESCRIPTION:\n",
    "\n",
    "This notebook creates a sparse matrix of size (#channels, #users). \n",
    "The value at position (channel_i, user_i) corresponds to the number of times user_i commented on channel_i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import scipy\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import zstandard as zstd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "scriptpath = \"../\"\n",
    "sys.path.append(os.path.abspath(scriptpath))\n",
    "from helpers.helpers_channels_more_10k import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set of duplicate users\n",
    "duplicate_users = dict_occurent_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numbers of users\n",
    "with open(\"/dlabdata1/youtube_large/jouven/channels_more_10k/nb_users.pkl\",'rb') as f:\n",
    "     nb_users = pickle.load(f)[0]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers dictionarries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionnary mapping the video_id to the channel_id\n",
    "vid_to_channels = video_id_to_channel_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selected channels and id-index mapping\n",
    "dict_channel_ind, dict_ind_channel, channels_id = filtered_channels_index_id_mapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the number of times each (channel, user) pairs appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function populates user_channels_occurences (dictionnary) containing the comments already processed for a \n",
    "given user\n",
    "\n",
    "PARAMETER:\n",
    "    - user_channels_occurences: dictionnary: {(channel index, user index): number of comments, ...}\n",
    "    - channel_idx: the index of the channel processed\n",
    "    - user_idx: the index of the user processed\n",
    "'''            \n",
    "def update_user_channel_occurences(user_channels_occurences, channel_idx, user_idx):\n",
    "    if (channel_idx, user_idx) in user_channels_occurences:\n",
    "        user_channels_occurences[(channel_idx, user_idx)] += 1\n",
    "    else:\n",
    "        user_channels_occurences[(channel_idx, user_idx)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line number: 75000000 time: 541.7293963432312\n",
      "line number: 150000000 time: 767.2881526947021\n"
     ]
    }
   ],
   "source": [
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=16384)\n",
    "\n",
    "# PARAMETERS\n",
    "\n",
    "# Dictionnary containing channel, user tuple\n",
    "graph_dict = {}\n",
    "# Dictionarray counting the numbers of comments for a given user\n",
    "user_channels_occurences = {}\n",
    "\n",
    "user = ''\n",
    "begin_time = time.time()\n",
    "# Numbers of channels\n",
    "nb_channels = len(channels_id)\n",
    "# Users idx\n",
    "user_idx = -1\n",
    "# Indices used for prints\n",
    "idx = 1\n",
    "nb = 1\n",
    "\n",
    "dir_1 = '/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/channels_more_10k'\n",
    "check_directory(dir_1)\n",
    "dir_2 = '/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/sparse_matrices_for_word2vecf'\n",
    "check_directory(dir_2)\n",
    "dir_3 = '/dlabdata1/youtube_large/jouven/channels_more_10k/users_index_mapping'\n",
    "check_directory(dir_3)\n",
    "\n",
    "\n",
    "# Create the training file where each line corresponds to a (word, context) = (channel, user) pairs for word2vecf\n",
    "f = open(\"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/channels_more_10k/training_data\", \"w\")\n",
    "f.close()\n",
    "train = \"\"\n",
    "train_temp = \"\"\n",
    "training_idx = 1\n",
    "\n",
    "\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_split = line.replace('\"', '').split(',')\n",
    "    if len(line_split) >= 9:\n",
    "        author_id = line_split[0]\n",
    "        if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "            channel_idx = dict_channel_ind[vid_to_channels[line_split[2]]]\n",
    "            \n",
    "            if author_id == user:\n",
    "            \n",
    "                if author_id in duplicate_users:\n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        update_user_channel_occurences(user_channels_occurences, channel_idx, user_idx)\n",
    "                        train_temp += str(channel_idx) + \" \" + str(user_idx) + \"\\n\"\n",
    "                        training_idx += 1\n",
    "                else:\n",
    "                    update_user_channel_occurences(user_channels_occurences, channel_idx, user_idx)\n",
    "                    train_temp += str(channel_idx) + \" \" + str(user_idx) + \"\\n\"\n",
    "                    training_idx += 1\n",
    "            else:\n",
    "                if len(user_channels_occurences) >= 1:\n",
    "                    # Update the dictionnary where data is stored\n",
    "                    graph_dict.update(user_channels_occurences)\n",
    "                \n",
    "                user_channels_occurences = {}\n",
    "                \n",
    "                if author_id in duplicate_users:\n",
    "                    duplicate_users[author_id] += 1\n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        user_idx += 1\n",
    "                        update_user_channel_occurences(user_channels_occurences, channel_idx, user_idx)\n",
    "                        train_temp += str(channel_idx) + \" \" + str(user_idx) + \"\\n\"\n",
    "                        training_idx += 1\n",
    "                        \n",
    "                else:\n",
    "                    user_idx += 1\n",
    "                    update_user_channel_occurences(user_channels_occurences, channel_idx, user_idx)\n",
    "                    train_temp += str(channel_idx) + \" \" + str(user_idx) + \"\\n\"\n",
    "                    training_idx += 1\n",
    "                    \n",
    "            user = author_id\n",
    "            \n",
    "        \n",
    "    if training_idx % 10000 == 0:\n",
    "        # For speed purpose\n",
    "        train = train + train_temp\n",
    "        train_temp = \"\"\n",
    "        \n",
    "    if training_idx % 75000000 == 0:\n",
    "        f = open(\"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/channels_more_10k/training_data\", \"a\")\n",
    "        f.write(train)\n",
    "        f.close()\n",
    "        train = \"\"\n",
    "        train_temp = \"\"\n",
    "                \n",
    "    if idx % 75000000 == 0:\n",
    "        # For space requirements every 75 millions line create a dok matrix and\n",
    "        # update it with the graph_dict dictionnary and then save it into csr format and then release memory\n",
    "        graph_matrix = dok_matrix((nb_channels, nb_users), np.uint32)\n",
    "        dict.update(graph_matrix, graph_dict)\n",
    "        graph_dict = {}\n",
    "        # Save sparse matrix\n",
    "        scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/sparse_matrices_for_word2vecf/matrice' + str(nb) + '.npz', graph_matrix.tocsr())\n",
    "        with open(\"/dlabdata1/youtube_large/jouven/sparse_matrix_construction/idx_bip.pkl\",'wb') as f:\n",
    "             pickle.dump([idx], f)\n",
    "        f.close()\n",
    "        graph_matrix = []\n",
    "        nb += 1\n",
    "        print('line number: ' + str(idx) + ' time: ' + str(time.time() - begin_time))\n",
    "        begin_time = time.time()\n",
    "        \n",
    "    idx += 1\n",
    "    \n",
    "\n",
    "# Store graph\n",
    "graph_matrix = dok_matrix((nb_channels, nb_users), np.uint32)\n",
    "dict.update(graph_matrix, graph_dict)\n",
    "graph_dict = {}\n",
    "scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/sparse_matrices_for_word2vecf/matrice' + str(nb) + '.npz', graph_matrix.tocsc())\n",
    "graph_matrix = []\n",
    "\n",
    "# Store (channel, user) data into the training file\n",
    "f = open(\"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/channels_more_10k/training_data\", \"a\")\n",
    "f.write(train)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lot of small sparse matrices have been produced because of space requirements. The final sparse matrice has to be generated from the sum of all sparse matrices constructed beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numbers of channels\n",
    "nb_channels = len(channels_id)\n",
    "\n",
    "\n",
    "graph = dok_matrix((nb_channels, nb_users), dtype = np.uint32).tocsr()\n",
    "path = '/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/word2vecf/'\n",
    "files = glob.glob(path + '*.npz')\n",
    "for file in files: \n",
    "    graph += scipy.sparse.load_npz(file).astype(np.uint32)\n",
    "    \n",
    "# Save the final sparse matrix\n",
    "scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/final_sparse_matrix/channels_more_10k/word2vecf/sparse_matrix_for_word2vec.npz', graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
