{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import scipy\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import zstandard as zstd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "scriptpath = \"../\"\n",
    "sys.path.append(os.path.abspath(scriptpath))\n",
    "from helpers.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set of duplicate users\n",
    "duplicate_users = dict_occurent_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers dictionarries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionnary mapping the video_id to the channel_id\n",
    "vid_to_channels = video_id_to_channel_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Channels that are in set_crawler dataset and also in which the language is in english\n",
    "dict_channel_ind, dict_ind_channel, channels_id = filtered_channels_index_id_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionnary mapping the channel index to the total number of comments in this channel\n",
    "with open(\"/dlabdata1/youtube_large/jouven/comments_per_channel_id.pkl\", 'rb') as f:\n",
    "    channels_idx_to_nb_comments = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the bipartite graph between the users and the channels using sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_channels_user_tuple(graph_dict, user_channels_occurences):\n",
    "    graph_dict.update(user_channels_occurences)\n",
    "\n",
    "            \n",
    "def update_user_channel_occurences(user_channels_occurences, channel_idx, user_idx):\n",
    "    if (channel_idx, user_idx) in user_channels_occurences:\n",
    "        user_channels_occurences[(channel_idx, user_idx)] += 1\n",
    "    else:\n",
    "        user_channels_occurences[(channel_idx, user_idx)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(graph_dict)  20518092\n",
      "20518092\n",
      "20518092\n",
      "line number: 75000000 time: 389.45528507232666\n",
      "len(graph_dict)  20638094\n",
      "20638094\n",
      "20638094\n",
      "line number: 150000000 time: 399.13372325897217\n",
      "len(graph_dict)  20510391\n",
      "20510391\n",
      "20510391\n",
      "line number: 225000000 time: 396.30357456207275\n",
      "len(graph_dict)  20456496\n",
      "20456496\n",
      "20456496\n",
      "line number: 300000000 time: 393.6120083332062\n",
      "len(graph_dict)  20528694\n",
      "20528694\n",
      "20528694\n",
      "line number: 375000000 time: 397.62460684776306\n",
      "len(graph_dict)  20584053\n",
      "20584053\n",
      "20584053\n",
      "line number: 450000000 time: 398.2904932498932\n",
      "len(graph_dict)  20564159\n",
      "20564159\n",
      "20564159\n",
      "line number: 525000000 time: 408.5987150669098\n",
      "len(graph_dict)  20444731\n",
      "20444731\n",
      "20444731\n",
      "line number: 600000000 time: 400.1595838069916\n",
      "len(graph_dict)  20602555\n",
      "20602555\n",
      "20602555\n",
      "line number: 675000000 time: 402.271853685379\n",
      "len(graph_dict)  20404779\n",
      "20404779\n",
      "20404779\n",
      "line number: 750000000 time: 409.7469153404236\n",
      "len(graph_dict)  20569439\n",
      "20569439\n",
      "20569439\n",
      "line number: 825000000 time: 419.4661660194397\n",
      "len(graph_dict)  20511771\n",
      "20511771\n",
      "20511771\n",
      "line number: 900000000 time: 395.14871311187744\n",
      "len(graph_dict)  20454989\n",
      "20454989\n",
      "20454989\n",
      "line number: 975000000 time: 399.60950112342834\n",
      "len(graph_dict)  20485761\n",
      "20485761\n",
      "20485761\n",
      "line number: 1050000000 time: 406.1157922744751\n",
      "len(graph_dict)  20579399\n",
      "20579399\n",
      "20579399\n",
      "line number: 1125000000 time: 405.015531539917\n",
      "len(graph_dict)  20577841\n",
      "20577841\n",
      "20577841\n",
      "line number: 1200000000 time: 421.88944149017334\n",
      "len(graph_dict)  20541603\n",
      "20541603\n",
      "20541603\n",
      "line number: 1275000000 time: 416.9858069419861\n",
      "len(graph_dict)  20539503\n",
      "20539503\n",
      "20539503\n",
      "line number: 1350000000 time: 415.6739549636841\n",
      "len(graph_dict)  20399249\n",
      "20399249\n",
      "20399249\n",
      "line number: 1425000000 time: 425.426127910614\n",
      "len(graph_dict)  20528615\n",
      "20528615\n",
      "20528615\n",
      "line number: 1500000000 time: 442.7096290588379\n",
      "len(graph_dict)  20536997\n",
      "20536997\n",
      "20536997\n",
      "line number: 1575000000 time: 422.6977174282074\n",
      "len(graph_dict)  20419364\n",
      "20419364\n",
      "20419364\n",
      "line number: 1650000000 time: 464.53707122802734\n",
      "len(graph_dict)  20546703\n",
      "20546703\n",
      "20546703\n",
      "line number: 1725000000 time: 414.064736366272\n",
      "len(graph_dict)  20427001\n",
      "20427001\n",
      "20427001\n",
      "line number: 1800000000 time: 421.7458670139313\n",
      "len(graph_dict)  20454619\n",
      "20454619\n",
      "20454619\n",
      "line number: 1875000000 time: 415.9938659667969\n",
      "len(graph_dict)  20476974\n",
      "20476974\n",
      "20476974\n",
      "line number: 1950000000 time: 412.9512484073639\n",
      "len(graph_dict)  20562348\n",
      "20562348\n",
      "20562348\n",
      "line number: 2025000000 time: 426.82577753067017\n",
      "len(graph_dict)  20564730\n",
      "20564730\n",
      "20564730\n",
      "line number: 2100000000 time: 435.26415157318115\n",
      "len(graph_dict)  20452188\n",
      "20452188\n",
      "20452188\n",
      "line number: 2175000000 time: 437.4682879447937\n",
      "len(graph_dict)  20458242\n",
      "20458242\n",
      "20458242\n",
      "line number: 2250000000 time: 427.19816732406616\n",
      "len(graph_dict)  20529044\n",
      "20529044\n",
      "20529044\n",
      "line number: 2325000000 time: 432.99264693260193\n",
      "len(graph_dict)  20549368\n",
      "20549368\n",
      "20549368\n",
      "line number: 2400000000 time: 421.86564898490906\n",
      "len(graph_dict)  20422570\n",
      "20422570\n",
      "20422570\n",
      "line number: 2475000000 time: 445.62899708747864\n",
      "len(graph_dict)  20439983\n",
      "20439983\n",
      "20439983\n",
      "line number: 2550000000 time: 419.9987008571625\n",
      "len(graph_dict)  20506146\n",
      "20506146\n",
      "20506146\n",
      "line number: 2625000000 time: 424.1973886489868\n",
      "len(graph_dict)  20542316\n",
      "20542316\n",
      "20542316\n",
      "line number: 2700000000 time: 424.44161081314087\n",
      "len(graph_dict)  20469587\n",
      "20469587\n",
      "20469587\n",
      "line number: 2775000000 time: 425.67268800735474\n",
      "len(graph_dict)  20391008\n",
      "20391008\n",
      "20391008\n",
      "line number: 2850000000 time: 421.64111590385437\n",
      "len(graph_dict)  20460062\n",
      "20460062\n",
      "20460062\n",
      "line number: 2925000000 time: 423.74589800834656\n",
      "len(graph_dict)  20488456\n",
      "20488456\n",
      "20488456\n",
      "line number: 3000000000 time: 430.94877004623413\n",
      "len(graph_dict)  20617424\n",
      "20617424\n",
      "20617424\n",
      "line number: 3075000000 time: 424.000611782074\n",
      "len(graph_dict)  20421421\n",
      "20421421\n",
      "20421421\n",
      "line number: 3150000000 time: 426.32849740982056\n",
      "len(graph_dict)  20506492\n",
      "20506492\n",
      "20506492\n",
      "line number: 3225000000 time: 422.2873876094818\n",
      "len(graph_dict)  20449081\n",
      "20449081\n",
      "20449081\n",
      "line number: 3300000000 time: 448.70649433135986\n",
      "len(graph_dict)  20600947\n",
      "20600947\n",
      "20600947\n",
      "line number: 3375000000 time: 424.46613097190857\n",
      "len(graph_dict)  20489106\n",
      "20489106\n",
      "20489106\n",
      "line number: 3450000000 time: 429.2955400943756\n",
      "len(graph_dict)  20474532\n",
      "20474532\n",
      "20474532\n",
      "line number: 3525000000 time: 430.8590290546417\n",
      "len(graph_dict)  20491477\n",
      "20491477\n",
      "20491477\n",
      "line number: 3600000000 time: 428.1350517272949\n",
      "len(graph_dict)  20536196\n",
      "20536196\n",
      "20536196\n",
      "line number: 3675000000 time: 420.43712043762207\n",
      "len(graph_dict)  20503921\n",
      "20503921\n",
      "20503921\n",
      "line number: 3750000000 time: 429.107257604599\n",
      "len(graph_dict)  20502389\n",
      "20502389\n",
      "20502389\n",
      "20565504\n",
      "line number: 3900000000 time: 441.82567286491394\n"
     ]
    }
   ],
   "source": [
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=16384)\n",
    "\n",
    "# PARAMETERS\n",
    "\n",
    "# Dictionnary containing channel, user tuple\n",
    "graph_dict = {}\n",
    "# Dictionarray counting the numbers of comments of a given user on channels\n",
    "user_channels_occurences = {}\n",
    "# Dictionnary giving to a user his corresponding index\n",
    "user_id_mapping = {}\n",
    "\n",
    "user = ''\n",
    "begin_time = time.time()\n",
    "# Numbers of channels\n",
    "nb_channels = len(channels_id)\n",
    "# Numbers of users\n",
    "nb_users = 406925230\n",
    "# Users idx\n",
    "user_idx = -1\n",
    "# Indices used for prints\n",
    "idx = 1\n",
    "nb = 1\n",
    "nb_user = 1\n",
    "\n",
    "# Create the training file where each line corresponds to a (word, context) = (channel, user) pairs for word2vecf\n",
    "f = open(\"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/training_data\", \"w\")\n",
    "f.close()\n",
    "train = \"\"\n",
    "train_temp = \"\"\n",
    "training_idx = 1\n",
    "\n",
    "dir_1 = '/dlabdata1/youtube_large/jouven/sparse_matrix_construction/sparse_matrices_for_word2vec'\n",
    "check_directory(dir_1)\n",
    "\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_split = line.replace('\"', '').split(',')\n",
    "    if len(line_split) == 9:\n",
    "        author_id = line_split[0]\n",
    "        if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "            channel_idx = dict_channel_ind[vid_to_channels[line_split[2]]]\n",
    "            \n",
    "            if author_id == user:\n",
    "                if author_id in duplicate_users:\n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        update_user_channel_occurences(user_channels_occurences, channel_idx, user_idx)\n",
    "                        train_temp += str(channel_idx) + \" \" + str(user_idx) + \"\\n\"\n",
    "                        training_idx += 1\n",
    "                else:\n",
    "                    update_user_channel_occurences(user_channels_occurences, channel_idx, user_idx)\n",
    "                    train_temp += str(channel_idx) + \" \" + str(user_idx) + \"\\n\"\n",
    "                    training_idx += 1\n",
    "            else:\n",
    "                if len(user_channels_occurences) > 0:\n",
    "                    add_channels_user_tuple(graph_dict, user_channels_occurences)\n",
    "                \n",
    "                user_channels_occurences = {}\n",
    "                \n",
    "                if author_id in duplicate_users:\n",
    "                    duplicate_users[author_id] += 1\n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        user_idx += 1\n",
    "                        update_user_channel_occurences(user_channels_occurences, channel_idx, user_idx)\n",
    "                        user_id_mapping[author_id] = user_idx\n",
    "                        train_temp += str(channel_idx) + \" \" + str(user_idx) + \"\\n\"\n",
    "                        training_idx += 1\n",
    "                        \n",
    "                else:\n",
    "                    user_idx += 1\n",
    "                    update_user_channel_occurences(user_channels_occurences, channel_idx, user_idx)\n",
    "                    user_id_mapping[author_id] = user_idx\n",
    "                    train_temp += str(channel_idx) + \" \" + str(user_idx) + \"\\n\"\n",
    "                    training_idx += 1\n",
    "                    \n",
    "    if len(user_id_mapping) % 25000000 == 0 and not len(user_id_mapping) == 0:\n",
    "        with open(\"/dlabdata1/youtube_large/jouven/users_index_mapping_comments_more_10k/user_mapping_\" + str(nb_user) +\".pkl\",'wb') as f:\n",
    "             pickle.dump(user_id_mapping, f)\n",
    "        f.close()\n",
    "        nb_user += 1\n",
    "        user_id_mapping = {}\n",
    "        \n",
    "    if training_idx % 10000 == 0:\n",
    "        # For speed purpose\n",
    "        train = train + train_temp\n",
    "        train_temp = \"\"\n",
    "        \n",
    "    if training_idx % 100000000 == 0:\n",
    "        f = open(\"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/training_data\", \"a\")\n",
    "        f.write(train)\n",
    "        f.close()\n",
    "        train = \"\"\n",
    "        train_temp = \"\"\n",
    "                \n",
    "    if idx % 75000000 == 0:\n",
    "        # For space requirements every 75 millions line create a dok matrix and\n",
    "        # update it with the graph_dict dictionnary and then save it into csr format and then release memory\n",
    "        graph_matrix = dok_matrix((nb_channels, nb_users), np.uint32)\n",
    "        dict.update(graph_matrix, graph_dict)\n",
    "        print('len(graph_dict) ', len(graph_dict))\n",
    "        print(graph_matrix.count_nonzero())\n",
    "        print(graph_matrix.tocsr().count_nonzero())\n",
    "        graph_dict = {}\n",
    "        # Save sparse matrix\n",
    "        scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/sparse_matrix_construction/sparse_matrices_for_word2vec/matrice' + str(nb) + '.npz', graph_matrix.tocsr())\n",
    "        with open(\"/dlabdata1/youtube_large/jouven/sparse_matrix_construction/idx_bip.pkl\",'wb') as f:\n",
    "             pickle.dump([idx], f)\n",
    "        f.close()\n",
    "        graph_matrix = []\n",
    "        nb += 1\n",
    "        print('line number: ' + str(idx) + ' time: ' + str(time.time() - begin_time))\n",
    "        begin_time = time.time()\n",
    "        \n",
    "    user = line_split[0]\n",
    "    idx += 1\n",
    "    \n",
    "\n",
    "# Store graph\n",
    "graph_matrix = dok_matrix((nb_channels, nb_users), np.uint32)\n",
    "dict.update(graph_matrix, graph_dict)\n",
    "graph_dict = {}\n",
    "scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/sparse_matrix_construction/sparse_matrices_for_word2vec/matrice' + str(nb) + '.npz', graph_matrix.tocsc())\n",
    "graph_matrix = []\n",
    "\n",
    "# Store user mapping\n",
    "with open(\"/dlabdata1/youtube_large/jouven/users_index_mapping_comments_more_10k/user_mapping_\" + str(nb_user) +\".pkl\",'wb') as f:\n",
    "     pickle.dump(user_id_mapping, f)\n",
    "f.close()\n",
    "user_id_mapping = {}\n",
    "\n",
    "# Store (channel, user) data into the training file\n",
    "f = open(\"/dlabdata1/youtube_large/jouven/word2vecf_preprocessing/training_data\", \"a\")\n",
    "f.write(train)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numbers of channels\n",
    "nb_channels = len(channels_id)\n",
    "# Numbers of users\n",
    "nb_users = 406925230\n",
    "\n",
    "graph = dok_matrix((nb_channels, nb_users), dtype = np.uint32).tocsr()\n",
    "path = '/dlabdata1/youtube_large/jouven/sparse_matrix_construction/sparse_matrices_for_word2vec/'\n",
    "files = glob.glob(path + '*.npz')\n",
    "for file in files: \n",
    "    graph += scipy.sparse.load_npz(file).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_2 = '/dlabdata1/youtube_large/jouven/final_sparse_matrix/'\n",
    "check_directory(dir_2)\n",
    "# Save the final sparse matrix\n",
    "scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/final_sparse_matrix/sparse_matrix_for_word2vec.npz', graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
