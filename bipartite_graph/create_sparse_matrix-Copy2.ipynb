{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import zstandard as zstd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "#from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnary mapping the video_id to the channel_id\n",
    "vid_to_channels = pd.read_pickle(\"/dlabdata1/youtube_large/id_to_channel_mapping.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channels that are in set_crawler dataset and also in which the language is in english\n",
    "with open('/dlabdata1/youtube_large/olam/channels_id_filtered.pickle', 'rb') as f:\n",
    "    channels_id = pickle.load(f)\n",
    "f.close()\n",
    "# Dictionnary to map the channel id to an integer corresponding to the column/row of the sparse matrix.\n",
    "channel_dict = {}\n",
    "channels_id = sorted(channels_id)\n",
    "for ind, channel_id in enumerate(channels_id):\n",
    "    channel_dict[channel_id] = ind\n",
    "channels_id = set(channels_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the bipartite graph between the users and the channels using sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using Zreader enables to read the .zst files line by line\n",
    "'''\n",
    "class Zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        '''Init method'''\n",
    "        self.fh = open(file,'rb')\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstd.ZstdDecompressor()\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def readlines(self):\n",
    "        '''Generator method that creates an iterator for each line of JSON'''\n",
    "        while True:\n",
    "            chunk = self.reader.read(self.chunk_size).decode(\"utf-8\", errors=\"ignore\")\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=16384)\n",
    "\n",
    "# parameters\n",
    "channel_to_nb_users_commented = {}\n",
    "idx = 1\n",
    "user_idx = 0\n",
    "user = ''\n",
    "begin_time = time.time()\n",
    "nb_users = 0\n",
    "\n",
    "#dir_1 = '/dlabdata1/youtube_large/jouven/channel_embedding/bipartite_sparse_matrix/'\n",
    "#check_directory(dir_1)\n",
    "\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_split = line.replace('\"', '').split(',')\n",
    "    \n",
    "    if idx > 1:\n",
    "        if len(line_split) == 9:\n",
    "            if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "                corr_channel = channel_dict[vid_to_channels[line_split[2]]]\n",
    "                if corr_channel in channel_to_nb_users_commented:\n",
    "                    channel_to_nb_users_commented[corr_channel] += 1\n",
    "                else:\n",
    "                    channel_to_nb_users_commented[corr_channel] = 1\n",
    "            if line_split[0] != user:\n",
    "                nb_users += 1\n",
    "    user = line_split[0]\n",
    "    \n",
    "                \n",
    "    idx += 1\n",
    "    if idx % 1000000000 == 0:\n",
    "        with open(\"/dlabdata1/youtube_large/jouven/idx_run.pkl\",'wb') as f:\n",
    "             pickle.dump([idx], f)\n",
    "        f.close()\n",
    "        \n",
    "with open(\"/dlabdata1/youtube_large/jouven/channels_idx_to_nb_comments.pkl\",'wb') as f:\n",
    "     pickle.dump(channel_to_nb_users_commented, f)\n",
    "f.close()\n",
    "with open(\"/dlabdata1/youtube_large/jouven/nb_users.pkl\",'wb') as f:\n",
    "     pickle.dump([nb_users], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52194"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(channel_to_nb_users_commented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_directory(dir_1):\n",
    "    \n",
    "    if not os.path.exists(dir_1): \n",
    "        os.makedirs(dir_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_edges(graph_dict, user, user_channels):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of graph dict: 36976\n"
     ]
    }
   ],
   "source": [
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=16384)\n",
    "\n",
    "# parameters\n",
    "max_freq = 0\n",
    "\n",
    "\n",
    "graph_dict = {}\n",
    "user_channels = []\n",
    "idx = 1\n",
    "user = ''\n",
    "begin_time = time.time()\n",
    "\n",
    "dir_1 = '/dlabdata1/youtube_large/jouven/channel_embedding/bipartite_sparse_matrix/'\n",
    "#check_directory(dir_1)\n",
    "\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_split = line.replace('\"', '').split(',')\n",
    "    if len(line_split) == 9:\n",
    "        if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "            corr_channel = vid_to_channels[line_split[2]]\n",
    "            if line_split[0] == user:\n",
    "                #user_channels.append(channel_dict[corr_channel])\n",
    "                graph_dict[line_split[0]] = idx\n",
    "                if len(graph_dict) == 1000:\n",
    "                    print('Size of graph dict: ' + str(sys.getsizeof(graph_dict)))\n",
    "                    break\n",
    "            else:\n",
    "                graph_dict[line_split[0]] = idx\n",
    "                if len(graph_dict) == 1000:\n",
    "                    print('Size of graph dict: ' + str(sys.getsizeof(graph_dict)))\n",
    "                    break\n",
    "                #create_edges(graph_dict_limited, user_channels)\n",
    "                #if len(graph_dict) >= 100000000:\n",
    "                    # For space requirements every 100 millions line create a dok matrix and\n",
    "                    # update it with the graph_dict dictionnary and then save it into csr format and then release memory\n",
    "                    #graph_matrix = dok_matrix((matrix_len, matrix_len), dtype=np.uint8)\n",
    "                    #dict.update(graph_matrix, graph_dict)\n",
    "                    #print('Size of matrix limited dok: ' + str(sys.getsizeof(graph_matrix)))\n",
    "                    #graph_dict_limited = {}\n",
    "                    # Save sparse matrix\n",
    "                    #scipy.sparse.save_npz('../../../dlabdata1/youtube_large/jouven/sparse_matrices_limited/matrice' + str(nb) + '.npz', graph_matrix.tocsr())\n",
    "\n",
    "                    #graph_matrix = []\n",
    "                    #nb += 1\n",
    "                    #print('line number: ' + str(idx) + ' time: ' + str(time.time() - begin_time))\n",
    "                    #begin_time = time.time()\n",
    "                \n",
    "                user_channels = []\n",
    "                user_channels.append(channel_dict[corr_channel])\n",
    "                \n",
    "    user = line_split[0]\n",
    "    idx += 1\n",
    "    \n",
    "\n",
    "#graph_matrix = dok_matrix((matrix_len, matrix_len), dtype=np.uint8)\n",
    "#dict.update(graph_matrix, graph_dict_limited)\n",
    "#print('Size of matrix limited dok: ' + str(sys.getsizeof(graph_matrix)))\n",
    "#graph_dict_limited = {}\n",
    "# Save sparse matrix\n",
    "#scipy.sparse.save_npz('../../../dlabdata1/youtube_large/jouven/sparse_matrices_limited/matrice' + str(nb) + '.npz', graph_matrix.tocsc())\n",
    "\n",
    "graph_matrix = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges_normalized(graph_dict, user_edge):\n",
    "    \n",
    "    nb_comments = len(user_edge)\n",
    "    user_edge = list(set(user_edge))\n",
    "    sorted_user_edge = sorted(user_edge)\n",
    "    \n",
    "    if len(user_edge) >= 2:\n",
    "        for comb in itertools.combinations(sorted_user_edge, 2):\n",
    "\n",
    "            user_edge = (comb[0], comb[1])\n",
    "            if user_edge in graph_dict:\n",
    "                graph_dict[user_edge] += 1/(math.log(nb_comments, 2))\n",
    "            else:\n",
    "                graph_dict[user_edge] = 1/(math.log(nb_comments, 2))\n",
    "\n",
    "            \n",
    "def create_edges_limited_normalized(graph_dict, user_edge):\n",
    "    \n",
    "    nb_comments = len(user_edge)\n",
    "    user_edge = list(set(user_edge))\n",
    "    if len(user_edge) > 100:\n",
    "        user_edge = random.sample(user_edge, 100)\n",
    "    \n",
    "    if len(user_edge) >= 2:\n",
    "        sorted_user_edge = sorted(user_edge)\n",
    "        for comb in itertools.combinations(sorted_user_edge, 2):\n",
    "            user_edge = (comb[0], comb[1])\n",
    "            if user_edge in graph_dict:\n",
    "                graph_dict[user_edge] += 1/(math.log(nb_comments, 2))\n",
    "            else:\n",
    "                graph_dict[user_edge] = 1/(math.log(nb_comments, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_directory(dir_1):\n",
    "    \n",
    "    if not os.path.exists(dir_1): \n",
    "        os.makedirs(dir_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/dlabdata1/youtube_large/jouven/occurent_users.pkl\",'rb') as f:\n",
    "    set_occurent_users = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_user_occurence = {}\n",
    "for val in set_occurent_users:\n",
    "    dict_user_occurence[val] = 0\n",
    "set_occurent_users = set(set_occurent_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=16384)\n",
    "\n",
    "# parameters\n",
    "graph_dict_limited = {}\n",
    "nb = 1\n",
    "idx = 1\n",
    "user_channels = []\n",
    "user_idx = -1\n",
    "user_bool = True\n",
    "\n",
    "user = ''\n",
    "begin_time = time.time()\n",
    "\n",
    "dir_1 = '/dlabdata1/youtube_large/jouven/sparse_matrix_construction/sparse_matrices_limited_normalized/'\n",
    "check_directory(dir_1)\n",
    "\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_split = line.replace('\"', '').split(',')\n",
    "    if len(line_split) == 9:\n",
    "        if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "            corr_channel = vid_to_channels[line_split[2]]\n",
    "            if user_idx in dict_user_occurence:\n",
    "                dict_user_occurence[user_idx] += 1\n",
    "                if dict_user_occurence[user_idx] > 1:\n",
    "                    user_bool = False\n",
    "            else:\n",
    "                user_bool = True\n",
    "                \n",
    "            if user_bool:\n",
    "                if line_split[0] == user:\n",
    "                    user_channels.append(channel_dict[corr_channel])\n",
    "                else:\n",
    "                    if len(user_channels) >= 2:\n",
    "                        create_edges_limited_normalized(graph_dict_limited, user_channels)\n",
    "                    #print(graph_dict_limited)\n",
    "                    if len(graph_dict_limited) >= 100000000:\n",
    "                        # For space requirements every 100 millions line create a dok matrix and\n",
    "                        # update it with the graph_dict dictionnary and then save it into csr format and then release memory\n",
    "                        graph_matrix = dok_matrix((matrix_len, matrix_len), dtype=np.uint8)\n",
    "                        dict.update(graph_matrix, graph_dict_limited)\n",
    "                        #print('Size of matrix limited dok: ' + str(sys.getsizeof(graph_matrix)))\n",
    "                        print(graph_matrix.count_nonzero())\n",
    "                        graph_dict_limited = {}\n",
    "                        # Save sparse matrix\n",
    "                        scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/sparse_matrix_construction/sparse_matrices_limited_normalized/matrice' + str(nb) + '.npz', graph_matrix.tocsr())\n",
    "                        with open(\"/dlabdata1/youtube_large/jouven/sparse_matrix_construction/idx.pkl\",'wb') as f:\n",
    "                             pickle.dump([idx], f)\n",
    "                        f.close()\n",
    "                        graph_matrix = []\n",
    "                        nb += 1\n",
    "                        print('line number: ' + str(idx) + ' time: ' + str(time.time() - begin_time))\n",
    "                        begin_time = time.time()\n",
    "\n",
    "                    user_channels = []\n",
    "                    user_channels.append(channel_dict[corr_channel])\n",
    "                    user_idx += 1\n",
    "            else:\n",
    "                if line_split[0] != user:\n",
    "                    user_idx += 1\n",
    "    user = line_split[0]\n",
    "    idx += 1\n",
    "    \n",
    "\n",
    "graph_matrix = dok_matrix((matrix_len, matrix_len), dtype=np.uint8)\n",
    "dict.update(graph_matrix, graph_dict_limited)\n",
    "print('Size of matrix limited dok: ' + str(sys.getsizeof(graph_matrix)))\n",
    "graph_dict_limited = {}\n",
    "# Save sparse matrix\n",
    "scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/sparse_matrix_construction/sparse_matrices_limited_normalized/matrice' + str(nb) + '.npz', graph_matrix.tocsr())\n",
    "\n",
    "graph_matrix = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
