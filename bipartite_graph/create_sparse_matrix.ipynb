{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTEBOOK DESCRIPTION:\n",
    "\n",
    "This notebook construct the bipartite graph between the channels and the users with tf-idf occurences using sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import scipy\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import zstandard as zstd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "scriptpath = \"/home/jouven/youtube_projects/\"\n",
    "sys.path.append(os.path.abspath(scriptpath))\n",
    "\n",
    "from helpers.helpers_channels_more_300 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of duplicate users\n",
    "duplicate_users = dict_occurent_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers dictionarries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionnary mapping the video_id to the channel_id\n",
    "vid_to_channels = video_id_to_channel_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channels that are in set_crawler dataset and also in which the language is in english\n",
    "dict_channel_ind, dict_ind_channel, channels_id = filtered_channels_index_id_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionnary mapping the channel index to the number of users that commented in this channel\n",
    "with open(\"/dlabdata1/youtube_large/jouven/channels_more_10k/nb_users_per_channel_more_10k.pkl\",'rb') as f:\n",
    "     nb_users_per_channel = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the bipartite graph between the users and the channels using sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the tf-idf weight between the channels that user_idx has commented and user_idx.\n",
    "\n",
    "PAREMETERS:\n",
    "    - graph_dict: dictionnary containing the edges already computed {channel index -> user index}\n",
    "    - user_idx: the user index that we consider\n",
    "    - user_channels_occurences: Dictionnary mapping the channels where user_idx has commented to, to \n",
    "    the number of comments user_idx did on this channel\n",
    "    - nb_users: Total number of users\n",
    "    \n",
    "'''\n",
    "def tf_idf_weight_occurences(graph_dict, user_idx, user_channels_occurences, nb_users):\n",
    "    max_freq = max(user_channels_occurences.values())\n",
    "    for channel in set(user_channels_occurences.keys()):\n",
    "        tf = user_channels_occurences[channel] / max_freq\n",
    "        idf = math.log(nb_users / nb_users_per_channel[channel])\n",
    "        graph_dict[(channel, user_idx)] = tf * idf\n",
    "        \n",
    "        \n",
    "'''\n",
    "Update the dictionnary `user_channels_occurences` keeping track of the occurences of the selected channels which are\n",
    "the channels that a user has commented.\n",
    "\n",
    "PARAMETERS:\n",
    "    - user_channels_occurences: Dictionnary: channel index -> # of times this channel has been called\n",
    "    - channel_idx: The channel index that we want to insert into user_channels_occurences\n",
    "'''\n",
    "def update_user_channel_occurences(user_channels_occurences, channel_idx):\n",
    "    # If the channel already exists in the dictionnary: update the # of occurence\n",
    "    if channel_idx in user_channels_occurences:\n",
    "        user_channels_occurences[channel_idx] += 1\n",
    "    else:\n",
    "        user_channels_occurences[channel_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line number: 365366768 time: 1870.8561975955963\n",
      "line number: 730660217 time: 1863.5573751926422\n"
     ]
    }
   ],
   "source": [
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=16384)\n",
    "\n",
    "# PARAMETERS\n",
    "\n",
    "# Dictionnary containing channel, user tuple\n",
    "graph_dict = {}\n",
    "# Dictionarray counting the numbers of comments of a given user on channels\n",
    "user_channels_occurences = {}\n",
    "\n",
    "user = ''\n",
    "begin_time = time.time()\n",
    "# Numbers of channels\n",
    "nb_channels = len(channels_id)\n",
    "# Numbers of users\n",
    "nb_users = 406925230\n",
    "# Users idx\n",
    "user_idx = -1\n",
    "# Indices used for prints\n",
    "idx = 1\n",
    "nb = 1\n",
    "\n",
    "dir_1 = '/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/sparse_matrices_bipartite_channel_more_10k'\n",
    "check_directory(dir_1)\n",
    "\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_split = line.replace('\"', '').split(',')\n",
    "    if len(line_split) == 9:\n",
    "        author_id = line_split[0]\n",
    "        if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "            channel_idx = dict_channel_ind[vid_to_channels[line_split[2]]]\n",
    "            \n",
    "            # If an edge from the same author is processed.\n",
    "            if author_id == user:\n",
    "                \n",
    "                if author_id in duplicate_users:\n",
    "                    # If the author is part of the set of duplicate users then we have to make sure\n",
    "                    # that it is the first time that we see that user \n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        update_user_channel_occurences(user_channels_occurences, channel_idx)\n",
    "                else:\n",
    "                    update_user_channel_occurences(user_channels_occurences, channel_idx)\n",
    "            else:\n",
    "                if len(user_channels_occurences) > 0:\n",
    "                    tf_idf_weight_occurences(graph_dict, user_idx, user_channels_occurences, nb_users)\n",
    "                \n",
    "                user_channels_occurences = {}\n",
    "                \n",
    "                # If the author is part of the set of duplicate users\n",
    "                if author_id in duplicate_users:\n",
    "                    # Add one to this user to indicate that this duplicate user has already been processed\n",
    "                    duplicate_users[author_id] += 1\n",
    "                    # we have to make sure that it is the first time that we see that user \n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        user_idx += 1\n",
    "                        update_user_channel_occurences(user_channels_occurences, channel_idx)\n",
    "                        \n",
    "                else:\n",
    "                    user_idx += 1\n",
    "                    update_user_channel_occurences(user_channels_occurences, channel_idx)\n",
    "                    \n",
    "                    \n",
    "\n",
    "    if len(graph_dict) >= 100000000:\n",
    "        # For space requirements every 100 millions line create a dok matrix and\n",
    "        # update it with the graph_dict dictionnary and then save it into csr format and then release memory\n",
    "        graph_matrix = dok_matrix((nb_channels, nb_users))\n",
    "        dict.update(graph_matrix, graph_dict)\n",
    "        graph_dict = {}\n",
    "        # Save sparse matrix\n",
    "        scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/sparse_matrices_bipartite_channel_more_10k/matrice' + str(nb) + '.npz', graph_matrix.tocsr())\n",
    "        with open(\"/dlabdata1/youtube_large/jouven/sparse_matrix_construction/idx_bip.pkl\",'wb') as f:\n",
    "             pickle.dump([idx], f)\n",
    "        f.close()\n",
    "        graph_matrix = []\n",
    "        nb += 1\n",
    "        print('line number: ' + str(idx) + ' time: ' + str(time.time() - begin_time))\n",
    "        begin_time = time.time()\n",
    "\n",
    "    user = line_split[0]\n",
    "    idx += 1\n",
    "    \n",
    "    \n",
    "\n",
    "graph_matrix = dok_matrix((nb_channels, nb_users))\n",
    "dict.update(graph_matrix, graph_dict)\n",
    "graph_dict = {}\n",
    "# Save sparse matrix\n",
    "scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/sparse_matrices_bipartite_channel_more_10k/matrice' + str(nb) + '.npz', graph_matrix.tocsc())\n",
    "graph_matrix = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we splitted the data corresponding to the bipartite graph in many small sparse matrices, we have to sum the sparse matrices to create a single one having all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_channels = len(channels_id)\n",
    "nb_users = 406925230\n",
    "\n",
    "graph = dok_matrix((nb_channels, nb_users)).tocsr()\n",
    "path = '/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/sparse_matrices_bipartite_channel_more_10k/'\n",
    "files = glob.glob(path + '*.npz')\n",
    "for file in files: \n",
    "    graph += scipy.sparse.load_npz(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final sparse matrix\n",
    "scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/final_embedding_sparse_matrix/channels_more_10k/sparse_matrix_bipartite_channels_more_10k.npz', graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
