{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTEBOOK DESCRIPTION:\n",
    "\n",
    "This notebook creates a sparse matrix which represent a proximity graph.\n",
    "Since the dataset is sorted according to each user we sequentially consider each user: \n",
    "\n",
    "\tFor each user:\n",
    "\t\tWe select the channels this user commented in.\n",
    "        And then, since our comments don’t have a time reference when it was posted we decided to construct edges of the graph by doing all the possible combinations out of the selected channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "import glob\n",
    "\n",
    "import zstandard as zstd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "scriptpath = \"/home/jouven/youtube_projects/\"\n",
    "sys.path.append(os.path.abspath(scriptpath))\n",
    "\n",
    "from helpers.helpers_channels_more_10k import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionnary mapping the video_id to the channel_id\n",
    "vid_to_channels = video_id_to_channel_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Channels with the selected comments\n",
    "dict_channel_ind, dict_ind_channel, channels_id = filtered_channels_index_id_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set of duplicate users\n",
    "duplicate_users = dict_occurent_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the goal of evaluating the the similarity between channels we created what we call a proximity graph.\n",
    "As the dataset is sorted according to each user we sequentially consider each user:\n",
    "For each user:\n",
    "\tWe select the channels this user commented in.\n",
    "\t\t\n",
    "And then, since our comments don’t have a time reference when it was posted we decided to construct edges of the graph by doing all the possible combinations out of the selected channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to add new edges to the existing data. Edges are formed by doing the 2-combinations of user_channels. \n",
    "If the edge already exists -> add 1 to the existing weight\n",
    "    PARAMETERS:\n",
    "        - graph_dict: dictionnary mapping the edge (tuple of channel indices) with the weight of that edge\n",
    "        - user_channels: set of channels that a user commented in\n",
    "'''\n",
    "\n",
    "def create_edges(graph_dict, user_channels):\n",
    "    \n",
    "    for comb in itertools.combinations(user_channels, 2):\n",
    "        edge = (comb[0], comb[1])\n",
    "        if edge in graph_dict:\n",
    "            graph_dict[edge] += 1\n",
    "        else:\n",
    "            graph_dict[edge] = 1\n",
    "            \n",
    "'''\n",
    "Function to add new edges to the existing data. Edges are formed by first selecting the context channels at random\n",
    "and them by doing the 2-combinations of user_channels. \n",
    "If the edge already exists -> add 1 to the existing weight\n",
    "    PARAMETERS:\n",
    "        - graph_dict: dictionnary mapping the edge (tuple of channel indices) with the weight of that edge\n",
    "        - user_channels: set of channels that a user commented in\n",
    "        - context: maximum number channels selected for a given user\n",
    "'''\n",
    "def create_edges_limited(graph_dict, user_channels, context):\n",
    "    \n",
    "    if len(user_channels) > context:\n",
    "        user_channels = random.sample(user_channels, context)\n",
    "    \n",
    "    for comb in itertools.combinations(user_channels, 2):\n",
    "        edge = (comb[0], comb[1])\n",
    "\n",
    "        if edge in graph_dict:\n",
    "            graph_dict[edge] += 1\n",
    "        else:\n",
    "            graph_dict[edge] = 1\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to add new edges to the existing data. Edges are formed by doing the 2-combinations of user_channels. \n",
    "If the edge already exists -> add 1/log(# number of channels that this user has commented in)\n",
    "    PARAMETERS:\n",
    "        - graph_dict: dictionnary mapping the edge (tuple of channel indices) with the weight of that edge\n",
    "        - user_channels: set of channels that a user commented in\n",
    "'''\n",
    "def create_edges_normalized(graph_dict, user_channels):\n",
    "    \n",
    "    nb_comments = len(user_channels)\n",
    "    \n",
    "    for comb in itertools.combinations(user_channels, 2):\n",
    "\n",
    "        edge = (comb[0], comb[1])\n",
    "        if edge in graph_dict:\n",
    "            graph_dict[edge] += 1/(math.log(nb_comments, 2))\n",
    "        else:\n",
    "            graph_dict[edge] = 1/(math.log(nb_comments, 2))\n",
    "\n",
    "'''\n",
    "Function to add new edges to the existing data. Edges are formed by first selecting the context channels at random\n",
    "and them by doing the 2-combinations of user_channels. \n",
    "If the edge already exists -> add 1/log(# number of channels that this user has commented in)\n",
    "    PARAMETERS:\n",
    "        - graph_dict: dictionnary mapping the edge (tuple of channel indices) with the weight of that edge\n",
    "        - user_channels: set of channels that a user commented in\n",
    "        - context: maximum number channels selected for a given user\n",
    "'''            \n",
    "def create_edges_limited_normalized(graph_dict, user_channels, context):\n",
    "    \n",
    "    nb_comments = len(user_channels)\n",
    "    if len(user_channels) > context:\n",
    "        user_channels = random.sample(user_channels, context)\n",
    "    \n",
    "    for comb in itertools.combinations(user_channels, 2):\n",
    "        edge = (comb[0], comb[1])\n",
    "        if edge in graph_dict:\n",
    "            graph_dict[edge] += 1/(math.log(nb_comments, 2))\n",
    "        else:\n",
    "            graph_dict[edge] = 1/(math.log(nb_comments, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line number: 137270327 time: 1158.5001480579376\n",
      "line number: 275022892 time: 1073.48898935318\n",
      "line number: 412639213 time: 1065.8698954582214\n",
      "line number: 549837858 time: 1062.13547539711\n",
      "line number: 687178507 time: 1079.4762296676636\n",
      "line number: 825136521 time: 1076.4855399131775\n",
      "line number: 962940178 time: 1063.9683725833893\n",
      "line number: 1100249592 time: 1062.4339764118195\n",
      "line number: 1237301359 time: 1085.1867506504059\n",
      "line number: 1374517380 time: 1076.8347342014313\n",
      "line number: 1512419854 time: 1054.8100345134735\n",
      "line number: 1650211681 time: 1053.404995918274\n",
      "line number: 1787434072 time: 1091.6070857048035\n",
      "line number: 1925929449 time: 1100.9692499637604\n",
      "line number: 2062947711 time: 1053.5515422821045\n",
      "line number: 2200687072 time: 1045.3022074699402\n",
      "line number: 2613319873 time: 1085.4445509910583\n",
      "line number: 2750808720 time: 1069.2701110839844\n",
      "line number: 2889148950 time: 1250.6160681247711\n",
      "line number: 3026568974 time: 1185.6369943618774\n",
      "line number: 3164283203 time: 1123.0101249217987\n",
      "line number: 3301917993 time: 1078.2657325267792\n"
     ]
    }
   ],
   "source": [
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=16384)\n",
    "\n",
    "# PARAMETERS\n",
    "\n",
    "# Dictionnary counting the number of time (channel_idx, channel2_idx) appears\n",
    "graph_dict_limited = {}\n",
    "# Indices\n",
    "nb = 1\n",
    "idx = 1\n",
    "# Channels that a user have commented\n",
    "user_channels = []\n",
    "# Number of channels, Row and columns length of the sparse matrix\n",
    "matrix_len = len(channels_id)\n",
    "context = 30\n",
    "\n",
    "user = ''\n",
    "begin_time = time.time()\n",
    "\n",
    "dir_1 = '/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/sparse_matrices_limited_normalized/'\n",
    "check_directory(dir_1)\n",
    "\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_split = line.replace('\"', '').split(',')\n",
    "    if len(line_split) >= 9:\n",
    "        author_id = line_split[0]\n",
    "        if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "            corr_channel = dict_channel_ind[vid_to_channels[line_split[2]]]\n",
    "            if author_id == user:\n",
    "                # if user is a duplicate user\n",
    "                if author_id in duplicate_users:\n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        user_channels.append(corr_channel)\n",
    "                else:\n",
    "                    user_channels.append(corr_channel)\n",
    "            else:\n",
    "                if len(user_channels) >= 2:\n",
    "                    create_edges_limited_normalized(graph_dict_limited, user_channels, context)\n",
    "                user_channels = []\n",
    "                \n",
    "                if len(graph_dict_limited) >= 75000000:\n",
    "                    # For space requirements every 75 millions line create a dok matrix and\n",
    "                    # update it with the graph_dict dictionnary and then save it into csr format and then release memory\n",
    "                    graph_matrix = dok_matrix((matrix_len, matrix_len))\n",
    "                    dict.update(graph_matrix, graph_dict_limited)\n",
    "                    graph_dict_limited = {}\n",
    "                    # Save sparse matrix\n",
    "                    scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/sparse_matrices_limited_normalized/matrice' + str(nb) + '.npz', graph_matrix.tocsr())\n",
    "                    with open(\"/dlabdata1/youtube_large/jouven/sparse_matrix_construction/idx2.pkl\",'wb') as f:\n",
    "                         pickle.dump([idx], f)\n",
    "                    f.close()\n",
    "                    graph_matrix = []\n",
    "                    nb += 1\n",
    "                    print('line number: ' + str(idx) + ' time: ' + str(time.time() - begin_time))\n",
    "                    begin_time = time.time()\n",
    "                        \n",
    "                # If user is a duplicate user\n",
    "                if author_id in duplicate_users:\n",
    "                    duplicate_users[author_id] += 1\n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        user_channels.append(corr_channel)\n",
    "                else:\n",
    "                    user_channels.append(corr_channel)\n",
    "           \n",
    "        user = author_id\n",
    "    idx += 1\n",
    "    \n",
    "\n",
    "# Store the graph sparse matrix\n",
    "graph_matrix = dok_matrix((matrix_len, matrix_len))\n",
    "dict.update(graph_matrix, graph_dict_limited)\n",
    "graph_dict_limited = {}\n",
    "# Save sparse matrix\n",
    "scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/sparse_matrices_limited_normalized/matrice' + str(nb) + '.npz', graph_matrix.tocsr())\n",
    "\n",
    "graph_matrix = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Enable to load every sparse matrix to form only one by adding the weights of all intermediate sparse matrix\n",
    "'''\n",
    "\n",
    "dir_1 = '/dlabdata1/youtube_large/jouven/final_sparse_matrix/channels_more_10k/proximity_graph'\n",
    "check_directory(dir_1)\n",
    "\n",
    "matrix_len = len(channels_id)\n",
    "\n",
    "path = '/dlabdata1/youtube_large/jouven/sparse_matrix_construction/channels_more_10k/sparse_matrices_limited_normalized/'\n",
    "files = glob.glob(path + '*.npz')\n",
    "\n",
    "graph = dok_matrix((matrix_len, matrix_len)).tocsr()\n",
    "for file in files: \n",
    "    graph += scipy.sparse.load_npz(file)\n",
    "# Save the final sparse matrix\n",
    "scipy.sparse.save_npz('/dlabdata1/youtube_large/jouven/final_sparse_matrix/channels_more_10k/proximity_graph/channel_by_channel_ln_30.npz', graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
