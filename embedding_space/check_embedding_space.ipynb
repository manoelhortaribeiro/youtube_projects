{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook checks the behaviour of the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import glob\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Retrieve the array obtained by apllying the dimentionality reduction algorithm\n",
    "graph_matrix: SHAPE: (channels, n_comp)\n",
    "\n",
    "PARAMETER:\n",
    "    - file_path: the path where the embedding graph is stored\n",
    "\n",
    "RETURN: \n",
    "    - df: DataFrame representing the graph in the embedding space\n",
    "'''\n",
    "def get_dataframe_in_embedding_space(file_path):\n",
    "    graph_matrix = np.load(file_path)\n",
    "    graph_matrix = graph_matrix['arr_0']\n",
    "    df = pd.DataFrame(graph_matrix)\n",
    "    df = df.rename(lambda x: 'dr'+str(x), axis='columns')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we have to map the index values into the channels_id thanks to the mapping dictionarray already created\n",
    "'''\n",
    "# Channels that are in set_crawler dataset and also in which the language is in english\n",
    "with open('/dlabdata1/youtube_large/olam/channels_id_filtered.pickle', 'rb') as f:\n",
    "    channels_id = pickle.load(f)\n",
    "f.close()\n",
    "channels_id = sorted(channels_id)\n",
    "# Dictionnary to map an integer corresponding to the column/row of the sparse matrix to the channel id.\n",
    "channel_index_dict = {}\n",
    "for ind, channel_id in enumerate(channels_id):\n",
    "    channel_index_dict[ind] = channel_id\n",
    "    \n",
    "# Dictionnary to map the channel id to an integer corresponding to the column/row of the sparse matrix.\n",
    "channel_id_dict = {}\n",
    "for ind, channel_id in enumerate(channels_id):\n",
    "    channel_id_dict[channel_id] = ind\n",
    "channels_id = set(channels_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find k closest channel using annoy library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First to check how good is the embedding space, we are going to choose a channel and it's k closest channels in the embedding space. By looking at these channels in the YouTube website, we should get similar channels if the embedding space is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.01815743, 2.50921109])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = AnnoyIndex(n_comp, \"euclidean\")  # Length of item vector that will be indexed\n",
    "df.apply(lambda row: index.add_item(row.name, np.array(row)), axis = 1)\n",
    "index.build(100) # 10 trees\n",
    "#index.save('../../../dlabdata1/youtube_large/jouven/annoy_index_10_trees.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item:  99347\n",
      "[99347, 126103, 156642, 116668, 4481, 149946, 107057, 81023, 126224, 55610, 20235, 46046, 114177, 52922, 90852, 21997, 96426, 55627, 120318, 3226]\n"
     ]
    }
   ],
   "source": [
    "#random.seed(1)\n",
    "item = random.randint(0, len(df))\n",
    "print('item: ', item)\n",
    "nearest_neighbors_index = index.get_nns_by_item(item, 20, search_k = len(df))\n",
    "print(nearest_neighbors_index) # will find the 1000 nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UCMXvZ5ki-b4X_wbHwsj7PZw',\n",
       " 'UCQu9eR8S7dAm1fdXBCOrJ6A',\n",
       " 'UCfoxf7UeCpFpBAguS3JwbPA',\n",
       " 'UCzTTM7g6KJ1lFF9wuJCdvTg',\n",
       " 'UCdYVjiqHCjpi2BR0PbTjmEg',\n",
       " 'UChI8VwnZuH6vcu1j8ld1Czw',\n",
       " 'UCMCiK6bN3PfdKvNnYJf_h2A',\n",
       " 'UCy3lCSLz5_LldHqNSeO7uMQ',\n",
       " 'UCR9ehrySGCcxFNg8gCA2rsQ',\n",
       " 'UCtI9t9l037t9KanXNtBl6mA',\n",
       " 'UCx3s3t5kpD4VMfJjDi5keXw',\n",
       " 'UCUgxI74QWFYEwNgDCIYFl3Q',\n",
       " 'UCMdHGrfjwVU3_Gq6Olk3sdA',\n",
       " 'UCAiomfxZAbfkyRSDs9gsiXg',\n",
       " 'UC6ePwIAVzpvlY_9x8mb1Z9A',\n",
       " 'UCaUajKAl3cpGQ6KARpnz_3w',\n",
       " 'UCuDcRrzB9aQde-6o8xEkJ7Q',\n",
       " 'UCGGTXs9gvw2ARbhOr5UE1sA',\n",
       " 'UCdtlhXq8cPkJ1evFkj9q5Qg',\n",
       " 'UCz7eke4JGlbtc11_6mmA7ew']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors_id = [channel_index_dict[val] for val in nearest_neighbors_index]\n",
    "nearest_neighbors_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>join_date</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>videos</th>\n",
       "      <th>channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Film and Animation</td>\n",
       "      <td>2017-05-21</td>\n",
       "      <td>http://www.youtube.com/channel/UCBJuEqXfXTdcPS...</td>\n",
       "      <td>MagnusNation</td>\n",
       "      <td>65100</td>\n",
       "      <td>28</td>\n",
       "      <td>UCBJuEqXfXTdcPSbGO9qqn1g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2011-12-13</td>\n",
       "      <td>http://www.youtube.com/channel/UCkNW9Q1VR_aeZ6...</td>\n",
       "      <td>Mago Dario Animazion...</td>\n",
       "      <td>60200</td>\n",
       "      <td>48</td>\n",
       "      <td>UCkNW9Q1VR_aeZ6uht83jJVQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Music</td>\n",
       "      <td>2013-09-13</td>\n",
       "      <td>http://www.youtube.com/channel/UC1xcnrpcF59FWW...</td>\n",
       "      <td>Mägo de Oz - Topic</td>\n",
       "      <td>40200</td>\n",
       "      <td>395</td>\n",
       "      <td>UC1xcnrpcF59FWWELtZvJTdg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Music</td>\n",
       "      <td>2008-03-17</td>\n",
       "      <td>http://www.youtube.com/channel/UCXhkGgooXHDNwg...</td>\n",
       "      <td>Mago Merlino</td>\n",
       "      <td>14800</td>\n",
       "      <td>838</td>\n",
       "      <td>UCXhkGgooXHDNwgJXmoTSN7g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2014-10-19</td>\n",
       "      <td>http://www.youtube.com/channel/UCvZGsuvKlYOGiZ...</td>\n",
       "      <td>MAGO TOMÁS</td>\n",
       "      <td>26200</td>\n",
       "      <td>31</td>\n",
       "      <td>UCvZGsuvKlYOGiZTsxwJNS5Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category   join_date  \\\n",
       "0  Film and Animation  2017-05-21   \n",
       "1       Entertainment  2011-12-13   \n",
       "2               Music  2013-09-13   \n",
       "3               Music  2008-03-17   \n",
       "4       Entertainment  2014-10-19   \n",
       "\n",
       "                                                link                     name  \\\n",
       "0  http://www.youtube.com/channel/UCBJuEqXfXTdcPS...             MagnusNation   \n",
       "1  http://www.youtube.com/channel/UCkNW9Q1VR_aeZ6...  Mago Dario Animazion...   \n",
       "2  http://www.youtube.com/channel/UC1xcnrpcF59FWW...       Mägo de Oz - Topic   \n",
       "3  http://www.youtube.com/channel/UCXhkGgooXHDNwg...             Mago Merlino   \n",
       "4  http://www.youtube.com/channel/UCvZGsuvKlYOGiZ...               MAGO TOMÁS   \n",
       "\n",
       "   subscribers  videos                channel_id  \n",
       "0        65100      28  UCBJuEqXfXTdcPSbGO9qqn1g  \n",
       "1        60200      48  UCkNW9Q1VR_aeZ6uht83jJVQ  \n",
       "2        40200     395  UC1xcnrpcF59FWWELtZvJTdg  \n",
       "3        14800     838  UCXhkGgooXHDNwgJXmoTSN7g  \n",
       "4        26200      31  UCvZGsuvKlYOGiZTsxwJNS5Q  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channelcrawler = pd.read_csv(\"/dlabdata1/youtube_large/channelcrawler.csv\")\n",
    "channelcrawler['channel_id'] = channelcrawler['link'].str.split('/').str[-1]\n",
    "channelcrawler.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Film and Animation', 'Entertainment', 'Music', 'Comedy', 'Gaming',\n",
       "       'Science & Technology', 'Sports', 'Education', 'People & Blogs',\n",
       "       'Nonprofits & Activism', 'Howto & Style', 'News & Politics',\n",
       "       'Travel & Events', 'Autos & Vehicles', 'Pets & Animals', nan],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channelcrawler['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>category</th>\n",
       "      <th>join_date</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>videos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCMXvZ5ki-b4X_wbHwsj7PZw</td>\n",
       "      <td>Film and Animation</td>\n",
       "      <td>2011-11-11</td>\n",
       "      <td>http://www.youtube.com/channel/UCMXvZ5ki-b4X_w...</td>\n",
       "      <td>Panoots</td>\n",
       "      <td>166263</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCQu9eR8S7dAm1fdXBCOrJ6A</td>\n",
       "      <td>Gaming</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>http://www.youtube.com/channel/UCQu9eR8S7dAm1f...</td>\n",
       "      <td>FIFA DAP</td>\n",
       "      <td>13600</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCfoxf7UeCpFpBAguS3JwbPA</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2014-12-16</td>\n",
       "      <td>http://www.youtube.com/channel/UCfoxf7UeCpFpBA...</td>\n",
       "      <td>play with me</td>\n",
       "      <td>69287</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCzTTM7g6KJ1lFF9wuJCdvTg</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2014-08-18</td>\n",
       "      <td>http://www.youtube.com/channel/UCzTTM7g6KJ1lFF...</td>\n",
       "      <td>Sensei Aishitemasu</td>\n",
       "      <td>40600</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCdYVjiqHCjpi2BR0PbTjmEg</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>2006-03-07</td>\n",
       "      <td>http://www.youtube.com/channel/UCdYVjiqHCjpi2B...</td>\n",
       "      <td>MattG124</td>\n",
       "      <td>469000</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UChI8VwnZuH6vcu1j8ld1Czw</td>\n",
       "      <td>Music</td>\n",
       "      <td>2009-05-12</td>\n",
       "      <td>http://www.youtube.com/channel/UChI8VwnZuH6vcu...</td>\n",
       "      <td>BelanovaVEVO</td>\n",
       "      <td>261721</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UCMCiK6bN3PfdKvNnYJf_h2A</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>2007-04-10</td>\n",
       "      <td>http://www.youtube.com/channel/UCMCiK6bN3PfdKv...</td>\n",
       "      <td>Se Joe</td>\n",
       "      <td>35900</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UCy3lCSLz5_LldHqNSeO7uMQ</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>2011-07-22</td>\n",
       "      <td>http://www.youtube.com/channel/UCy3lCSLz5_LldH...</td>\n",
       "      <td>brandon begin</td>\n",
       "      <td>47000</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UCR9ehrySGCcxFNg8gCA2rsQ</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2013-10-02</td>\n",
       "      <td>http://www.youtube.com/channel/UCR9ehrySGCcxFN...</td>\n",
       "      <td>Hermetic Kitten ASMR...</td>\n",
       "      <td>300000</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UCtI9t9l037t9KanXNtBl6mA</td>\n",
       "      <td>Film and Animation</td>\n",
       "      <td>2011-09-19</td>\n",
       "      <td>http://www.youtube.com/channel/UCtI9t9l037t9Ka...</td>\n",
       "      <td>allspark2013</td>\n",
       "      <td>17000</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>UCx3s3t5kpD4VMfJjDi5keXw</td>\n",
       "      <td>Science &amp; Technology</td>\n",
       "      <td>2011-12-25</td>\n",
       "      <td>http://www.youtube.com/channel/UCx3s3t5kpD4VMf...</td>\n",
       "      <td>CoolHardLogic</td>\n",
       "      <td>106000</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>UCUgxI74QWFYEwNgDCIYFl3Q</td>\n",
       "      <td>Music</td>\n",
       "      <td>2018-07-10</td>\n",
       "      <td>http://www.youtube.com/channel/UCUgxI74QWFYEwN...</td>\n",
       "      <td>Tisakorean - Topic</td>\n",
       "      <td>15800</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>UCMdHGrfjwVU3_Gq6Olk3sdA</td>\n",
       "      <td>Education</td>\n",
       "      <td>2013-09-10</td>\n",
       "      <td>http://www.youtube.com/channel/UCMdHGrfjwVU3_G...</td>\n",
       "      <td>Five Little Monkeys ...</td>\n",
       "      <td>33400</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>UCAiomfxZAbfkyRSDs9gsiXg</td>\n",
       "      <td>Film and Animation</td>\n",
       "      <td>2015-04-08</td>\n",
       "      <td>http://www.youtube.com/channel/UCAiomfxZAbfkyR...</td>\n",
       "      <td>Princely</td>\n",
       "      <td>113000</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UC6ePwIAVzpvlY_9x8mb1Z9A</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>http://www.youtube.com/channel/UC6ePwIAVzpvlY_...</td>\n",
       "      <td>Eric Beckerman</td>\n",
       "      <td>216000</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>UCaUajKAl3cpGQ6KARpnz_3w</td>\n",
       "      <td>Education</td>\n",
       "      <td>2014-02-05</td>\n",
       "      <td>http://www.youtube.com/channel/UCaUajKAl3cpGQ6...</td>\n",
       "      <td>English For You</td>\n",
       "      <td>577739</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>UCuDcRrzB9aQde-6o8xEkJ7Q</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>http://www.youtube.com/channel/UCuDcRrzB9aQde-...</td>\n",
       "      <td>Sebby Clemens</td>\n",
       "      <td>11497</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>UCGGTXs9gvw2ARbhOr5UE1sA</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>2013-02-07</td>\n",
       "      <td>http://www.youtube.com/channel/UCGGTXs9gvw2ARb...</td>\n",
       "      <td>Love The Climb VLOG</td>\n",
       "      <td>13300</td>\n",
       "      <td>573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>UCdtlhXq8cPkJ1evFkj9q5Qg</td>\n",
       "      <td>Music</td>\n",
       "      <td>2009-11-10</td>\n",
       "      <td>http://www.youtube.com/channel/UCdtlhXq8cPkJ1e...</td>\n",
       "      <td>MyJESUStv</td>\n",
       "      <td>35200</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>UCz7eke4JGlbtc11_6mmA7ew</td>\n",
       "      <td>Music</td>\n",
       "      <td>2015-01-24</td>\n",
       "      <td>http://www.youtube.com/channel/UCz7eke4JGlbtc1...</td>\n",
       "      <td>Falcom Music Channel...</td>\n",
       "      <td>23800</td>\n",
       "      <td>10027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  channel_id              category   join_date  \\\n",
       "0   UCMXvZ5ki-b4X_wbHwsj7PZw    Film and Animation  2011-11-11   \n",
       "1   UCQu9eR8S7dAm1fdXBCOrJ6A                Gaming  2018-02-01   \n",
       "2   UCfoxf7UeCpFpBAguS3JwbPA         Entertainment  2014-12-16   \n",
       "3   UCzTTM7g6KJ1lFF9wuJCdvTg         Entertainment  2014-08-18   \n",
       "4   UCdYVjiqHCjpi2BR0PbTjmEg                Comedy  2006-03-07   \n",
       "5   UChI8VwnZuH6vcu1j8ld1Czw                 Music  2009-05-12   \n",
       "6   UCMCiK6bN3PfdKvNnYJf_h2A                Comedy  2007-04-10   \n",
       "7   UCy3lCSLz5_LldHqNSeO7uMQ        People & Blogs  2011-07-22   \n",
       "8   UCR9ehrySGCcxFNg8gCA2rsQ         Entertainment  2013-10-02   \n",
       "9   UCtI9t9l037t9KanXNtBl6mA    Film and Animation  2011-09-19   \n",
       "10  UCx3s3t5kpD4VMfJjDi5keXw  Science & Technology  2011-12-25   \n",
       "11  UCUgxI74QWFYEwNgDCIYFl3Q                 Music  2018-07-10   \n",
       "12  UCMdHGrfjwVU3_Gq6Olk3sdA             Education  2013-09-10   \n",
       "13  UCAiomfxZAbfkyRSDs9gsiXg    Film and Animation  2015-04-08   \n",
       "14  UC6ePwIAVzpvlY_9x8mb1Z9A         Entertainment  2015-07-31   \n",
       "15  UCaUajKAl3cpGQ6KARpnz_3w             Education  2014-02-05   \n",
       "16  UCuDcRrzB9aQde-6o8xEkJ7Q         Entertainment  2015-10-29   \n",
       "17  UCGGTXs9gvw2ARbhOr5UE1sA        People & Blogs  2013-02-07   \n",
       "18  UCdtlhXq8cPkJ1evFkj9q5Qg                 Music  2009-11-10   \n",
       "19  UCz7eke4JGlbtc11_6mmA7ew                 Music  2015-01-24   \n",
       "\n",
       "                                                 link  \\\n",
       "0   http://www.youtube.com/channel/UCMXvZ5ki-b4X_w...   \n",
       "1   http://www.youtube.com/channel/UCQu9eR8S7dAm1f...   \n",
       "2   http://www.youtube.com/channel/UCfoxf7UeCpFpBA...   \n",
       "3   http://www.youtube.com/channel/UCzTTM7g6KJ1lFF...   \n",
       "4   http://www.youtube.com/channel/UCdYVjiqHCjpi2B...   \n",
       "5   http://www.youtube.com/channel/UChI8VwnZuH6vcu...   \n",
       "6   http://www.youtube.com/channel/UCMCiK6bN3PfdKv...   \n",
       "7   http://www.youtube.com/channel/UCy3lCSLz5_LldH...   \n",
       "8   http://www.youtube.com/channel/UCR9ehrySGCcxFN...   \n",
       "9   http://www.youtube.com/channel/UCtI9t9l037t9Ka...   \n",
       "10  http://www.youtube.com/channel/UCx3s3t5kpD4VMf...   \n",
       "11  http://www.youtube.com/channel/UCUgxI74QWFYEwN...   \n",
       "12  http://www.youtube.com/channel/UCMdHGrfjwVU3_G...   \n",
       "13  http://www.youtube.com/channel/UCAiomfxZAbfkyR...   \n",
       "14  http://www.youtube.com/channel/UC6ePwIAVzpvlY_...   \n",
       "15  http://www.youtube.com/channel/UCaUajKAl3cpGQ6...   \n",
       "16  http://www.youtube.com/channel/UCuDcRrzB9aQde-...   \n",
       "17  http://www.youtube.com/channel/UCGGTXs9gvw2ARb...   \n",
       "18  http://www.youtube.com/channel/UCdtlhXq8cPkJ1e...   \n",
       "19  http://www.youtube.com/channel/UCz7eke4JGlbtc1...   \n",
       "\n",
       "                       name  subscribers  videos  \n",
       "0                   Panoots       166263      28  \n",
       "1                  FIFA DAP        13600      75  \n",
       "2              play with me        69287      19  \n",
       "3        Sensei Aishitemasu        40600     788  \n",
       "4                  MattG124       469000     608  \n",
       "5              BelanovaVEVO       261721      35  \n",
       "6                    Se Joe        35900     770  \n",
       "7             brandon begin        47000      71  \n",
       "8   Hermetic Kitten ASMR...       300000     530  \n",
       "9              allspark2013        17000     195  \n",
       "10            CoolHardLogic       106000      51  \n",
       "11       Tisakorean - Topic        15800      30  \n",
       "12  Five Little Monkeys ...        33400     192  \n",
       "13                 Princely       113000     548  \n",
       "14           Eric Beckerman       216000     113  \n",
       "15          English For You       577739     233  \n",
       "16            Sebby Clemens        11497      61  \n",
       "17      Love The Climb VLOG        13300     573  \n",
       "18                MyJESUStv        35200     176  \n",
       "19  Falcom Music Channel...        23800   10027  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors = pd.DataFrame(nearest_neighbors_id, columns = ['channel_id']).merge(channelcrawler)\n",
    "nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channels selected over the whole comments dataset\n",
    "We randomly choose 10 000 users over the dataset.\n",
    "For each user, we then pick two channels at random in the set of channels this user commented in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionnary mapping the video_id to the channel_id\n",
    "vid_to_channels = pd.read_pickle(\"/dlabdata1/youtube_large/id_to_channel_mapping.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        '''Init method'''\n",
    "        self.fh = open(file,'rb')\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstd.ZstdDecompressor()\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def readlines(self):\n",
    "        '''Generator method that creates an iterator for each line of JSON'''\n",
    "        while True:\n",
    "            chunk = self.reader.read(self.chunk_size).decode(\"utf-8\", errors=\"ignore\")\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Zreader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-50b724b6e0f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16384\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Zreader' is not defined"
     ]
    }
   ],
   "source": [
    "def add_channels(user_c):\n",
    "    return random.sample(user_c, 2)\n",
    "\n",
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=16384)\n",
    "\n",
    "# parameters\n",
    "len_random_set = 10001\n",
    "nb_users = 534744094\n",
    "nb = 1\n",
    "user_channels = []\n",
    "channels_tuple = []\n",
    "threshold = (len_random_set*1.1) / nb_users\n",
    "not_completed = False\n",
    "\n",
    "user = ''\n",
    "\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_split = line.replace('\"', '').split(',')\n",
    "    if user != line_split[0]:\n",
    "        if not_completed:\n",
    "            if len(user_channels) >= 2:\n",
    "                channels_tuple.append(tuple(add_channels(user_channels)))\n",
    "                nb += 1\n",
    "                not_completed = False\n",
    "            user_channels = []\n",
    "        else:\n",
    "            user_channels = []\n",
    "            if random.random() <= threshold:\n",
    "                if len(line_split) == 9:\n",
    "                    if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "                        corr_channel = vid_to_channels[line_split[2]]\n",
    "                        user_channels.append(channel_id_dict[corr_channel])\n",
    "                not_completed = True\n",
    "        \n",
    "    else:\n",
    "        if len(line_split) == 9:\n",
    "            if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "                corr_channel = vid_to_channels[line_split[2]]\n",
    "                if channel_id_dict[corr_channel] not in set(user_channels):\n",
    "                    user_channels.append(channel_id_dict[corr_channel])\n",
    "    user = line_split[0]\n",
    "\n",
    "\n",
    "    if nb == len_random_set:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/dlabdata1/youtube_large/jouven/randomly_selected_channels.pkl\",'wb') as f:\n",
    "     pickle.dump(channels_tuple, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_walk(df_embedding):\n",
    "    random_values_1 = random.sample(range(len_embedding), k = len_random_set)\n",
    "    random_values_2 = random.sample(range(len_embedding), k = len_random_set)\n",
    "    random_walk_channels = list(zip(random_values_1, random_values_2))\n",
    "\n",
    "    random_walk_distance = 0\n",
    "    for val in random_walk_channels:\n",
    "        random_walk_distance += distance.euclidean(df_embedding.iloc[val[0]], df_embedding.iloc[val[1]])\n",
    "    return random_walk_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics: users walk distance and relative nearest neighbor ranking \n",
    "\n",
    "In this section we want to measure the euclidian distance of a user walk compared to a random walk.\n",
    "\n",
    "user walk: Euclidean distance in the embedding space between two randomly channels of a user.\n",
    "random walk: Euclidean distance in the embedding space between two randomly channels.\n",
    "position: Position of a channel taken from user u relatively of another channel taken from the same user in terms of its nearest neighbor ranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take only 10000 elements from the 100000 elements channels tuple\n",
    "#channels_tuple2 = random.sample(channels_tuple, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#channels_tuple = random.sample(channels_tuple3, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the list of channels tuple in order to compute the user walk and the position\n",
    "#with open(\"/dlabdata1/youtube_large/jouven/randomly_selected_channels_filtered.pkl\", 'wb') as f:\n",
    "#    pickle.dump(channels_tuple, f)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the list of channels tuple in order to compute the user walk and the position\n",
    "with open(\"/dlabdata1/youtube_large/jouven/randomly_selected_channels_filtered.pkl\", 'rb') as f:\n",
    "    channels_tuple = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_tuple = random.sample(channels_tuple, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get the position of ref_channel relative to second_channel in terms of its nearest neighbors ranking.\n",
    "PARAMETER:\n",
    "    - ref_channel: The reference channel on which wwe compute it's k nearest neighbor\n",
    "    - second_channel: The channel where we compute it's ranking relatively to ref_channel\n",
    "    - dist: Euclidean distance between ref_channel and second_channel\n",
    "    - index: annoy index\n",
    "    - df_embedding: DataFrame representing the embedding space\n",
    "\n",
    "RETURN: The position of second_channel relatively to ref_channel in terms of it's ranking\n",
    "\n",
    "'''\n",
    "def get_ranking_position_between_channels(ref_channel, second_channel, dist, index, df_embedding):\n",
    "    #print('ref ', ref_channel)\n",
    "    #print('second ', second_channel)\n",
    "    #print('dist ', dist)\n",
    "    \n",
    "    # Number of nearest neighbor we are looking for\n",
    "    \n",
    "    \n",
    "    # length of last iteration\n",
    "    len_last_it = 0\n",
    "    \n",
    "    nearest_neighbors_index = index.get_nns_by_item(ref_channel, len(df_embedding), search_k = 100000000)\n",
    "    dist_k_th_nearest = distance.euclidean(df_embedding.iloc[ref_channel], \n",
    "                                           df_embedding.iloc[nearest_neighbors_index[len(nearest_neighbors_index)-1]])\n",
    "    for i in range(0, len(nearest_neighbors_index)):\n",
    "        if nearest_neighbors_index[i] == second_channel:\n",
    "            return i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Retrieve the array obtained by apllying the dimentinality reductin algorithm\n",
    "    graph_matrix: SHAPE: (channels, n_comp)\n",
    "    \n",
    "    PARAMETERS:\n",
    "        - df_embedding: DataFrame representing the graph in the embedding space\n",
    "        - n_comp: number of components to use after the dimentionalit reduction\n",
    "        \n",
    "    RETURN: The annoy index\n",
    "    '''\n",
    "\n",
    "def get_annoy_index(df_embedding, n_comp):\n",
    "    \n",
    "    index = AnnoyIndex(n_comp, \"euclidean\")  # Length of item vector that will be indexed\n",
    "    df_embedding.apply(lambda row: index.add_item(row.name, np.array(row)), axis = 1)\n",
    "    index.build(100) # 100 trees\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file  /dlabdata1/youtube_large/jouven/channel_embedding/limited_normalized_50/reduced_pca_100.npz\n",
      "reduced_pca_100.npz\n",
      "n_comp  100\n",
      "file  /dlabdata1/youtube_large/jouven/channel_embedding/limited_normalized_50/reduced_pca_50.npz\n",
      "reduced_pca_50.npz\n",
      "n_comp  50\n",
      "file  /dlabdata1/youtube_large/jouven/channel_embedding/limited_normalized_50/reduced_pca_200.npz\n",
      "reduced_pca_200.npz\n",
      "n_comp  200\n"
     ]
    }
   ],
   "source": [
    "users_walk_tab = []\n",
    "ranking_position_tab = []\n",
    "\n",
    "len_random_set = 5000\n",
    "len_embedding = len(channels_id)\n",
    "\n",
    "path = '/dlabdata1/youtube_large/jouven/channel_embedding/limited_normalized_50/'\n",
    "files = glob.glob(path + '*.npz')   \n",
    "for file in files: \n",
    "    print('file ', file)\n",
    "    search_n_comp = file.split(\"/\")\n",
    "    print(search_n_comp[6])\n",
    "    n_comp = int(re.findall('[0-9]+', search_n_comp[6])[0])\n",
    "    print('n_comp ', n_comp)\n",
    "    df_embedding = get_dataframe_in_embedding_space(file)\n",
    "    random_walk_distance = get_random_walk(df_embedding)\n",
    "        \n",
    "    index = get_annoy_index(df_embedding, n_comp)\n",
    "\n",
    "    users_walk = 0\n",
    "    ranking_position = 0\n",
    "\n",
    "    for ref_channel, second_channel in channels_tuple:\n",
    "        dist = distance.euclidean(df_embedding.iloc[ref_channel], df_embedding.iloc[second_channel])\n",
    "        users_walk += dist\n",
    "        ranking_position += get_ranking_position_between_channels(ref_channel, second_channel, dist, index, df_embedding)\n",
    "    \n",
    "    users_walk_tab.append(users_walk/random_walk_distance)\n",
    "    ranking_position_tab.append(ranking_position/(len(channels_tuple)*len(channels_id)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155930"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embedding.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.884401161727789, 0.9240368724033946, 0.937289456175224]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_walk_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39859200025652536, 0.39952344128775735, 0.39865621368562815]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_position_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Plot the results obtained when computing the metrics\n",
    "'''\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "components_name = ['Features dim 50', 'Features dim 100', 'Features dim 200', 'Features dim 500']\n",
    "colors = ['b', 'g,', 'r', 'c']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(components_name)):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=ranking_position_tab,\n",
    "        y=users_walk_tab,\n",
    "        marker=dict(color=colors[i], size=12),\n",
    "        mode=\"markers\",\n",
    "        name=components_name[i],\n",
    "    ))\n",
    "\n",
    "fig.update_layout(title=\"Gender Earnings Disparity\",\n",
    "                  xaxis_title=\"Ranking position\",\n",
    "                  yaxis_title=\"Users walk distance\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
