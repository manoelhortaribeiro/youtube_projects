{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTEBOOK DESCRIPTIONS:\n",
    "\n",
    "This notebook is considered as a helper notebook to create:\n",
    "- The set of users that appeared multiple times in the `comments_dataset` file\n",
    "- Compute the number of comments per channel\n",
    "- Filter the video-channel relationship dataframe with the selected channels\n",
    "- Compute the number of users after filtered the set of channels\n",
    "- Compute random/user jumper pairs of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import scipy.sparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "scriptpath = \"/home/jouven/youtube_projects/\"\n",
    "sys.path.append(os.path.abspath(scriptpath))\n",
    "\n",
    "from helpers.helpers_channels_more_300 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video channel mapping filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the original dictionnary having all the video-channel relationship, we only select data corresponding to the selected channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected channels and id-index mapping\n",
    "dict_channel_ind, dict_ind_channel, channels_id = filtered_channels_index_id_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_to_channels = pd.read_pickle(\"/dlabdata1/youtube_large/id_to_channel_mapping.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original length of the relationship dataframe ', len(vid_to_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of channels selected\n",
    "dict_channel_ind, dict_ind_channel, channels_id = filtered_channels_index_id_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_to_channels_filtered = {}\n",
    "for vid, channel in vid_to_channels.items():\n",
    "    if not dict_channel_ind.get(channel) == None:\n",
    "        if channel in channels_id:\n",
    "            vid_to_channels_filtered[vid] = channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filtered length of the relationship dataframe ', len(vid_to_channels_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the  video_id to the channel index filtered mapping\n",
    "with open(\"/dlabdata1/youtube_large/jouven/channels_more_300/video_to_channel_mapping_filtered.pkl\",'wb') as f:\n",
    "     pickle.dump(vid_to_channels_filtered, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some users appear to have duplicate rows and the goal is to find these duplicate rows to delete them when reading the `comments_dataset` file.\n",
    "\n",
    "In order to find these duplicate users, we first read the whole `comments_dataset` file and retrieve each user for each block of comments. Since this file is ordered by user, each time we encounter a different user we put it in the created dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected channels and id-index mapping\n",
    "dict_channel_ind, dict_ind_channel, channels_id = filtered_channels_index_id_mapping()\n",
    "\n",
    "# Dictionnary mapping the video_id to the channel_id\n",
    "vid_to_channels = video_id_to_channel_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1276141994 line have been processed\n",
      "2552907222 line have been processed\n",
      "3829866804 line have been processed\n",
      "5105730512 line have been processed\n",
      "6382036099 line have been processed\n",
      "7659055277 line have been processed\n",
      "8935401407 line have been processed\n",
      "10210173875 line have been processed\n",
      "10365014154 line have been processed\n"
     ]
    }
   ],
   "source": [
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=160384)\n",
    "\n",
    "# parameters\n",
    "idx = 1\n",
    "comments_per_channel = {}\n",
    "user = ''\n",
    "begin_time = time.time()\n",
    "# Users having commented\n",
    "users = []\n",
    "nb = 0\n",
    "\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_split = line.replace('\"', '').split(',')\n",
    "    if len(line_split) >= 9:\n",
    "        author_id = line_split[0]\n",
    "        if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "            if author_id != user:\n",
    "                users.append(author_id)\n",
    "            \n",
    "    if len(users) >= 50000000:\n",
    "        print(str(idx) + ' line have been processed')\n",
    "        with open(\"/dlabdata1/youtube_large/jouven/idx.pkl\",'wb') as f:\n",
    "             pickle.dump([idx], f)\n",
    "        f.close()\n",
    "        df = pd.DataFrame({'users': users})\n",
    "        if nb == 0:\n",
    "            df.to_csv('/dlabdata1/youtube_large/jouven/channels_more_300/users.csv.gz', compression='gzip', index = False)\n",
    "        else:\n",
    "            df.to_csv('/dlabdata1/youtube_large/jouven/channels_more_300/users.csv.gz', compression='gzip', mode='a', index = False, header = False)\n",
    "        nb += 1\n",
    "        df = 0\n",
    "        users = []\n",
    "        \n",
    "    user = author_id\n",
    "    idx += 1\n",
    "    \n",
    "print(str(idx) + ' line have been processed')\n",
    "with open(\"/dlabdata1/youtube_large/jouven/idx.pkl\",'wb') as f:\n",
    "     pickle.dump([idx], f)\n",
    "f.close()\n",
    "df = pd.DataFrame({'users': users})\n",
    "if nb == 0:\n",
    "    df.to_csv('/dlabdata1/youtube_large/jouven/channels_more_300/users.csv.gz', compression='gzip', index = False)\n",
    "else:\n",
    "    df.to_csv('/dlabdata1/youtube_large/jouven/channels_more_300/users.csv.gz', compression='gzip', mode='a', index = False, header = False)\n",
    "nb += 1\n",
    "df = 0\n",
    "users = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the created dataframe, we find the duplicate users by looking at the duplicates users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('/dlabdata1/youtube_large/jouven/channels_more_300/users.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users = len(users.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379473215"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of users ', nb_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/dlabdata1/youtube_large/jouven/channels_more_300/nb_users.pkl\",'wb') as f:\n",
    "     pickle.dump([nb_users], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_users = users[users.duplicated() == True].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17825"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of users having duplicate rows =  ', users - nb_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/dlabdata1/youtube_large/jouven/channels_more_300/duplicate_users.pkl\",'wb') as f:\n",
    "     pickle.dump(list(duplicate_users['users']), f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of comments per channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of comments per channel by traversing the whole `comments_dataset` and store it into a dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of duplicate users\n",
    "duplicate_users = dict_occurent_users()\n",
    "\n",
    "# Selected channels and id-index mapping\n",
    "dict_channel_ind, dict_ind_channel, channels_id = filtered_channels_index_id_mapping()\n",
    "\n",
    "# Dictionnary mapping the video_id to the channel_id\n",
    "vid_to_channels = video_id_to_channel_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function add a comment into the a dictionnary keeping track of the number of comments per channel\n",
    "PARAMETERS:\n",
    "    - dictionnary: dicionnary mapping the channel index with it's corresponding number of comments\n",
    "    - corr_channel: the channel index in which we want to add a comment\n",
    "'''\n",
    "def add_comment(dictionnary, corr_channel):\n",
    "    if corr_channel in dictionnary:\n",
    "        dictionnary[corr_channel] += 1\n",
    "    else:\n",
    "        dictionnary[corr_channel] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6333.590427398682\n",
      "-6144.500817298889\n",
      "-6062.126186847687\n",
      "-5905.280533075333\n",
      "-6172.949314117432\n",
      "-6818.088992357254\n",
      "-6077.464460372925\n",
      "-5907.094409942627\n",
      "-5780.842119693756\n",
      "-5753.591685295105\n"
     ]
    }
   ],
   "source": [
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=160384)\n",
    "\n",
    "# parameters\n",
    "idx = 1\n",
    "comments_per_channel = {}\n",
    "user = ''\n",
    "begin_time = time.time()\n",
    "\n",
    "\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_split = line.replace('\"', '').split(',')\n",
    "    if len(line_split) == 9:\n",
    "        author_id = line_split[0]\n",
    "        if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "            corr_channel = vid_to_channels[line_split[2]]\n",
    "            if author_id == user:\n",
    "                if author_id in duplicate_users:\n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        add_comment(comments_per_channel, corr_channel)\n",
    "                else:\n",
    "                    add_comment(comments_per_channel, corr_channel)\n",
    "            else:\n",
    "                if author_id in duplicate_users:\n",
    "                    duplicate_users[author_id] += 1\n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        add_comment(comments_per_channel, corr_channel)\n",
    "                else:\n",
    "                    add_comment(comments_per_channel, corr_channel)\n",
    "            \n",
    "            user = author_id\n",
    "    idx += 1\n",
    "    if idx % 1000000000 == 0:\n",
    "        print(begin_time-time.time())\n",
    "        begin_time = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the comments_per_channel dictionnary into the YouTube project\n",
    "with open(\"/dlabdata1/youtube_large/jouven/comments_per_channel.pkl\",'wb') as f:\n",
    "     pickle.dump(comments_per_channel, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the pairs of channels for the user and random jumper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_SAMPLE = 3000\n",
    "\n",
    "# Load the channel tuple sparse matrix\n",
    "S = scipy.sparse.load_npz('/dlabdata1/youtube_large/jouven/final_sparse_matrix/channels_more_300/sparse_matrix_word2vec_users_commented_geq_2_channels.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the 3000 pairs of channels with the new random jumper method.\n",
    "For every pair, we take 2 users at random and then for each user we select one channel at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_users = np.random.choice(np.arange(S.shape[1]), NB_SAMPLE, replace=False)\n",
    "S2 = S[:, selected_users_rw]\n",
    "\n",
    "# Create and store channels tuples\n",
    "channels_tuple = []\n",
    "last = 0\n",
    "for i in range(S2.shape[1]):\n",
    "    idx = S2[:, i].nonzero()\n",
    "    idx = idx[0]\n",
    "    \n",
    "    if i % 2 == 1:\n",
    "        selected_channel = np.random.choice(idx, 1)\n",
    "        channels_tuple.append((last, selected_channel))\n",
    "    else:\n",
    "        last = np.random.choice(idx, 1)\n",
    "    \n",
    "with open(\"/dlabdata1/youtube_large/jouven/channels_more_300/channels_tuple_random_walk_new.pkl\",'wb') as f:\n",
    "     pickle.dump(channels_tuple, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of the random pairs of channels to compute the random jumper of a model.\n",
    "From the set of channels that we have, we select a pair of channels by selecting at random 2 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_tuple = []\n",
    "for val in selected_channels_rw:\n",
    "    channels_tuple.append((val[0], val[1]))\n",
    "with open(\"/dlabdata1/youtube_large/jouven/channels_more_300/channels_tuple_random_walk.pkl\",'wb') as f:\n",
    "     pickle.dump(channels_tuple, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of the random pairs to compute the user jumper of a model.\n",
    "We select a pair of channel by selecting a user at random and then selecting random 2 channels from this user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample NB_SAMPLE from the set of users\n",
    "selected_users = np.random.randint(S.shape[1] - 1, size=NB_SAMPLE)\n",
    "S = S[:, selected_users]\n",
    "\n",
    "# Create and store channels tuples\n",
    "channels_tuple = []\n",
    "for i in range(S.shape[1]):\n",
    "    idx = S[:, i].nonzero()\n",
    "    idx = idx[0]\n",
    "    \n",
    "    selected_channels = np.random.choice(idx, 2, replace=False)\n",
    "    channels_tuple.append((selected_channels[0], selected_channels[1]))\n",
    "    \n",
    "with open(\"/dlabdata1/youtube_large/jouven/channels_more_300/channels_tuple_user_walk.pkl\",'wb') as f:\n",
    "     pickle.dump(channels_tuple, f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
