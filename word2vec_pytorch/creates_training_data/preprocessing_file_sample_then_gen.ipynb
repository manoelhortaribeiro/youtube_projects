{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTEBOOK DESCRIPTION:\n",
    "\n",
    "This notebook builds the training data for the word2vec_pytorch implementation. \n",
    "It builds the training data by selecting at most CONTEXT channels from a user and then performing all the combinations between the selected channels\n",
    "\n",
    "WARNING: Before running this notebook, `/word2vecf/config.py` needs to be filled with the wanted parameters(THRESHOLD_NAME) corresponding to the minimum number of comments per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import permutations, combinations\n",
    "\n",
    "scriptpath = \"/home/jouven/youtube_projects/word2vec_pytorch/\"\n",
    "sys.path.append(os.path.abspath(scriptpath))\n",
    "from config import *\n",
    "\n",
    "\n",
    "scriptpath = \"/home/jouven/youtube_projects/\"\n",
    "sys.path.append(os.path.abspath(scriptpath))\n",
    "from helpers.helpers_channels_more_300 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create training data from reader the comments dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build the training data, the following code reads the orignal `comments_dataset`, as usual we process sequentially each user and puts the results into a Pandas DataFrame. \n",
    "The results need to have a specific format: every line of the DataFrame needs to contain pairs of channels corresponding to the (input, output) of a given user.\n",
    "    \n",
    "    For each user:\n",
    "        - Select the channels that this user has commented\n",
    "        - Perform subsampling if specified\n",
    "        - Select at maximum CONTEXT channels from the set of channels this user has commented \n",
    "        - Perform the 2-combinations out of the select channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COMMON_PATH = \"/dlabdata1/youtube_large/jouven/word2vec_pytorch/channels_more_\" + THRESHOLD_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionnary mapping the video_id to the channel_id\n",
    "vid_to_channels = video_id_to_channel_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set of duplicate users\n",
    "duplicate_users = dict_occurent_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Channels with the selected comments\n",
    "dict_channel_ind, dict_ind_channel, channels_id = filtered_channels_index_id_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function selects at maximum CONTEXT channels from the set of channels this user has commented in, performs\n",
    "the 2-combinations out the selected channels and append the results to the data.\n",
    "\n",
    "PARAMETERS:\n",
    "    - data: List containing pairs of channels corresponding to the users already processed\n",
    "    - user_channels: The list of channel a given user has commented in\n",
    "'''\n",
    "def create_pairs(data, user_channels):\n",
    "    if len(user_channels) > CONTEXT_SIZE:\n",
    "        user_channels = random.sample(user_channels, CONTEXT_SIZE)\n",
    "    for comb in itertools.combinations(user_channels, 2):\n",
    "        data.append((comb[0], comb[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create training set ...\n",
      "idx 76827\n",
      "nb 1\n",
      "idx 155879\n",
      "nb 2\n",
      "idx 247193\n",
      "nb 3\n",
      "idx 333841\n",
      "nb 4\n",
      "idx 403207\n",
      "nb 5\n",
      "idx 483070\n",
      "nb 6\n",
      "idx 564200\n",
      "nb 7\n",
      "idx 643304\n",
      "nb 8\n",
      "idx 736036\n",
      "nb 9\n",
      "idx 813628\n",
      "nb 10\n",
      "idx 897520\n",
      "nb 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-20fbc126a60a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Create training set ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Read each line from the reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mline_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_split\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/youtube_projects/helpers/helpers_channels_more_300.py\u001b[0m in \u001b[0;36mreadlines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m'''Generator method that creates an iterator for each line of JSON'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specific\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=16384)\n",
    "\n",
    "# PARAMETERS\n",
    "\n",
    "# Dictionnary counting the number of time (channel_idx, channel2_idx) appears\n",
    "data = []\n",
    "# Indices\n",
    "nb = 0\n",
    "idx = 1\n",
    "# Channels that a user have commented\n",
    "user_channels = []\n",
    "# Number of channels, Row and columns length of the sparse matrix\n",
    "matrix_len = len(channels_id)\n",
    "\n",
    "# Create directory if not existing\n",
    "check_directory(COMMON_DLAB_PATH)\n",
    "\n",
    "user = ''\n",
    "begin_time = time.time()\n",
    "\n",
    "\n",
    "if SUBSAMPLING:\n",
    "    print('performing subsampling ...')\n",
    "    \n",
    "    with open(os.path.join(COMMON_PATH, \"vocab_occ.pkl\"),'rb') as f:\n",
    "        vocab_occ = pickle.load(f)\n",
    "    f.close()\n",
    "    total = np.sum(vocab_occ) # Total number of comments\n",
    "    \n",
    "    selected_channels = []\n",
    "    for channel in range(len(vocab_occ)):\n",
    "        frac = vocab_occ[channel]/total\n",
    "        prob = 1 - np.sqrt(SAMPLING_RATE/frac)\n",
    "\n",
    "        sampling = np.random.sample()\n",
    "        if (sampling >= prob):\n",
    "            selected_channels.append(channel)\n",
    "        selected_channels = set(selected_channels)\n",
    "\n",
    "print('Create training set ...')\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_split = line.replace('\"', '').split(',')\n",
    "    if len(line_split) >= 9:\n",
    "        author_id = line_split[0]\n",
    "        if vid_to_channels.get(line_split[2]) in channels_id:\n",
    "            corr_channel = dict_channel_ind[vid_to_channels[line_split[2]]]\n",
    "            if author_id == user:\n",
    "                # if user is a duplicate user\n",
    "                if author_id in duplicate_users:\n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        user_channels.append(corr_channel)\n",
    "                else:\n",
    "                    user_channels.append(corr_channel)\n",
    "            else:\n",
    "                if SUBSAMPLING:\n",
    "                    user_channels = list(set(user_channels).intersection(selected_channels))\n",
    "                else:\n",
    "                    user_channels = list(set(user_channels))\n",
    "                \n",
    "                # We need at list 2 channels to build a line into the training set.\n",
    "                if len(user_channels) >= 2:\n",
    "                    create_pairs(data, user_channels)\n",
    "                user_channels = []\n",
    "                \n",
    "                # For memory purpose add results to the DataFrame\n",
    "                if len(data) >= 50000000:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    if nb == 0:\n",
    "                        df.to_csv(TRAINING_DATA_PATH, compression='gzip', index = False)\n",
    "                    else:\n",
    "                        df.to_csv(TRAINING_DATA_PATH, compression='gzip', mode='a', index = False, header = False)\n",
    "                    nb += 1\n",
    "                    data = []\n",
    "                    df = 0\n",
    "                    print('idx ' + str(idx))\n",
    "                    print('nb ' + str(nb))\n",
    "                    \n",
    "                # If user is a duplicate user\n",
    "                if author_id in duplicate_users:\n",
    "                    duplicate_users[author_id] += 1\n",
    "                    if duplicate_users[author_id] <= 1:\n",
    "                        user_channels.append(corr_channel)\n",
    "                else:\n",
    "                    user_channels.append(corr_channel)\n",
    "           \n",
    "        user = author_id\n",
    "    idx += 1\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(TRAINING_DATA_PATH, compression='gzip', mode='a', index = False, header = False)\n",
    "data = 0\n",
    "df = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
