{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterising the evolution of the user interests on YT\n",
    "\n",
    "This notebook covers the whole pipeline of the project.\n",
    "\n",
    "Here is the table content of the principal tasks : \n",
    "- **Data Processing**\n",
    "- **Preparing Pyspark Model**\n",
    "- **Topic modelling**\n",
    "- **Results: Topic Coherence**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/olam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import fasttext\n",
    "import gzip\n",
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType, StructField, StructType\n",
    "from pyspark.ml.clustering import LDA, LDAModel, LocalLDAModel\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing \n",
    "\n",
    "In this section, we will select the data for the topic modelling model as follow:\n",
    "\n",
    "- videos has at least 10'000 views\n",
    "- videos from channels with at least 100'000 subscribers\n",
    "\n",
    "For that, we proceed as follow :\n",
    "\n",
    "- first pass over the whole dataset in order to build the vocabulary and the keep the index of the relevant videos\n",
    "- second pass over the whole dataset to construct a NxM sparse matrix, where N is the number of videos and M is the number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50456 relevant channels.\n"
     ]
    }
   ],
   "source": [
    "# GET THE LIST OF RELEVANT CHANNELS\n",
    "\n",
    "df_channelcrawler = pd.read_csv('/dlabdata1/youtube_large/channelcrawler.csv')\n",
    "\n",
    "df_channelcrawler['channel_id'] = df_channelcrawler['link'].apply(\n",
    "    lambda x: x.replace('http://www.youtube.com/channel/', ''))\n",
    "\n",
    "# Filter channels with at least 100'000 subs\n",
    "df_channelcrawler = df_channelcrawler[df_channelcrawler['subscribers'] >= 100000]\n",
    "\n",
    "# Store in a set since it will be faster to check if a channel is in channelcrawler\n",
    "set_relevant_channels = set(df_channelcrawler['channel_id'])\n",
    "\n",
    "print('There are ' + str(len(set_relevant_channels)) + ' relevant channels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting NLP pre-processing features\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_10000_views(video):\n",
    "    try:\n",
    "        return video['view_count'] >= 10000\n",
    "    except KeyError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_channel(video):\n",
    "    try:\n",
    "        return video['channel_id'] in set_relevant_channels\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglishAlpha(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_tokens_per_video(video):\n",
    "    ''''''\n",
    "\n",
    "    title_tokens = [w for w in tokenizer.tokenize(\n",
    "        video['title'].lower()) if not w in stop_words]\n",
    "    tag_tokens = [w for w in tokenizer.tokenize(\n",
    "        video['tags'].lower()) if not w in stop_words]\n",
    "\n",
    "    # We want to keep duplicates !!\n",
    "    tokens_per_video = title_tokens + tag_tokens\n",
    "\n",
    "    # Filter token with length < 3, with non english alphabet since fastext is not 100% accurate and remove numerical token\n",
    "    tokens_keep = []\n",
    "    for token in tokens_per_video:\n",
    "        if len(token) >= 3 and (not token.isnumeric()) and isEnglishAlpha(token):\n",
    "            tokens_keep.append(token)\n",
    "\n",
    "    # Stemming\n",
    "    stemmed_tokens_per_video = ([s_stemmer.stem(w) for w in tokens_keep])\n",
    "\n",
    "    # Return a Counter object of the tokens\n",
    "    return collections.Counter(stemmed_tokens_per_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pass \n",
    "The first pass on the dataset will allow us to recover relevant videos and list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Error -3 while decompressing data: invalid code lengths set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-db926ef8d560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/dlabdata1/youtube_large/yt_metadata_en_dd.jsonl.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             \u001b[0muncompress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: Error -3 while decompressing data: invalid code lengths set"
     ]
    }
   ],
   "source": [
    "# Variable that contains the idx of every non english vid and that\n",
    "# belongs to a channel in channelcrawler.csv TO BE USED IN SECOND ITER\n",
    "set_relevant_vid = set()\n",
    "\n",
    "# Variable first instanciated as set to check existing tokens efficiently,\n",
    "# which will be a list in order to get the index for each tokens\n",
    "set_stemmed_tokens = set()\n",
    "\n",
    "# Reading the file\n",
    "with gzip.open('/dlabdata1/youtube_large/yt_metadata_en_dd.jsonl.gz', 'rb') as f:\n",
    "\n",
    "    for i, line in enumerate(f):\n",
    "\n",
    "        try:\n",
    "            # line is a byte dict, video is the corresponding dict\n",
    "            video = json.loads(line)\n",
    "        except:\n",
    "            video = {'channel_id': None}\n",
    "\n",
    "        if check_channel(video) and check_10000_views(video):\n",
    "\n",
    "            tokens_per_video = get_freq_tokens_per_video(video)\n",
    "\n",
    "            set_relevant_vid.add(i)\n",
    "            set_stemmed_tokens.update(tokens_per_video)\n",
    "\n",
    "        if i % 1000000 == 0 and i != 0:\n",
    "            print('Processed ' + str(i) + ' videos...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save some intermediate results\n",
    "\n",
    "with open('/dlabdata1/youtube_large/olam/data/final_res/set_relevant_vid.pickle', 'wb') as f:\n",
    "    pickle.dump(set_relevant_vid, f)\n",
    "f.close()\n",
    "\n",
    "with open('/dlabdata1/youtube_large/olam/data/final_res/set_stemmed_tokens.pickle', 'wb') as f:\n",
    "    pickle.dump(set_stemmed_tokens, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second pass \n",
    "The second pass on the dataset will allow us to create a NxM sparse matrix, where N is the number of videos and M is the number of words in the vocabulary\n",
    "\n",
    "Notes:\n",
    "- In order to keep the memory usage low, we will fill sparse matrix with only 1'000'000 rows and save them in the csr format\n",
    "- Stack the saved sparse matrix together to get the final sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_rows(M):\n",
    "    '''Function that removes all rows from sparse matrix M that contains only zero.'''\n",
    "    num_nonzeros = np.diff(M.indptr)\n",
    "    return M[num_nonzeros != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_underlying_dict(freq_tokens_per_video, word2id, i_vid):\n",
    "    '''Method to fill the underlying dictionnary in order to \n",
    "    update the sparse matrix incrementally by videos'''\n",
    "\n",
    "    dict_freq_tokens_for_sparse_matrix = {}\n",
    "\n",
    "    for key in freq_tokens_per_video.keys():\n",
    "\n",
    "        # Column index in the sparse matrix (one column for each token)\n",
    "        try:\n",
    "            j_token = word2id[key]\n",
    "\n",
    "            # Filling the underlying dict\n",
    "            dict_freq_tokens_for_sparse_matrix[(\n",
    "                i_vid % 1000000, j_token)] = freq_tokens_per_video[key]\n",
    "\n",
    "        except KeyError:\n",
    "            None\n",
    "\n",
    "    return dict_freq_tokens_for_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimension of sparse matrix\n",
    "size_of_tokens_dict = len(set_stemmed_tokens)\n",
    "number_of_vid = len(set_relevant_vid)\n",
    "\n",
    "# Create dictionnary of tokens with their indice\n",
    "word2id = {}\n",
    "\n",
    "# Fill dictionnary of tokens\n",
    "for i, token in enumerate(set_stemmed_tokens):\n",
    "    word2id[token] = i\n",
    "    \n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "# Create mini sparse matrix\n",
    "S = dok_matrix((1000000, size_of_tokens_dict), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_vid = 0\n",
    "\n",
    "# Reading the file\n",
    "with gzip.open('/dlabdata1/youtube_large/yt_metadata_en_dd.jsonl.gz', 'rb') as f:\n",
    "\n",
    "    for i, line in enumerate(f):\n",
    "\n",
    "        if i_vid % 1000000 == 0 and i_vid != 0:\n",
    "\n",
    "            # Transform to csr format for memory efficiency\n",
    "            S = S.tocsr()\n",
    "            file_name = 'S' + str(int(i_vid/1000000)) + '.npz'\n",
    "\n",
    "            if not os.path.isfile('/dlabdata1/youtube_large/olam/data/final_res/matrices/' + file_name):\n",
    "                scipy.sparse.save_npz(\n",
    "                    '/dlabdata1/youtube_large/olam/data/final_res/matrices/' + file_name, S)\n",
    "\n",
    "                # Refresh mini sparse matrix\n",
    "                S = dok_matrix((1000000, size_of_tokens_dict), dtype=np.uint8)\n",
    "\n",
    "        if i in set_relevant_vid:\n",
    "\n",
    "            video = json.loads(line)\n",
    "\n",
    "            # Get the tokens for each video and theirs number of occurences\n",
    "            freq_tokens_per_video = get_freq_tokens_per_video(video)\n",
    "\n",
    "            # Fill the underlying dict\n",
    "            dict_freq_tokens_for_sparse_matrix = fill_underlying_dict(\n",
    "                freq_tokens_per_video, word2id, i_vid)\n",
    "\n",
    "            # Fill data in to sparse matrix\n",
    "            dict.update(S, dict_freq_tokens_for_sparse_matrix)\n",
    "\n",
    "            # Increase i_vid\n",
    "            i_vid += 1\n",
    "\n",
    "        if i % 1000000 == 0 and i != 0:\n",
    "            print('Processed ' + str(i) + ' videos...')\n",
    "\n",
    "# Save last sparse matrix\n",
    "S = S.tocsr()\n",
    "S = remove_zero_rows(S)\n",
    "scipy.sparse.save_npz(\n",
    "    '/dlabdata1/youtube_large/olam/data/final_res/matrices/S_last.npz', S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final sparse matrix\n",
    "S = scipy.sparse.load_npz(\n",
    "    '/dlabdata1/youtube_large/olam/data/final_res/matrices/S1.npz')\n",
    "\n",
    "for i in range(2, XXX):\n",
    "    S_next = scipy.sparse.load_npz(\n",
    "        '/dlabdata1/youtube_large/olam/data/final_res/matrices/S' + str(i) + '.npz')\n",
    "    S = scipy.sparse.vstack([S, S_next])\n",
    "\n",
    "# Add last matrix\n",
    "S_last = scipy.sparse.load_npz(\n",
    "    '/dlabdata1/youtube_large/olam/data/final_res/matrices/S_last.npz')\n",
    "S = scipy.sparse.vstack([S, S_last])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the full matrix\n",
    "scipy.sparse.save_npz(\n",
    "    '/dlabdata1/youtube_large/olam/data/final_res/matrices/S_full.npz', S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data for Topic Modelling with PySpark\n",
    "\n",
    "First, for a better topic modelling model, we select the videos from the sparse matrix as follow:\n",
    "- we group the videos by their respective `channel_id`, `category` and `upload_date`\n",
    "- keep the 20 videos with the most `view_counts` from each group\n",
    "- keep tokens that appears in at least 100 videos\n",
    "\n",
    "Then, we should compute a spark dataframe from our final sparse matrix in order to compute the models on the hadoop cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the videos for topic modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas DataFrame of relevant videos with relevant features\n",
    "columns_names = ['idx', 'channel_id',\n",
    "                 'view_counts', 'uploaded_year', 'category']\n",
    "\n",
    "# store relevant features of relevant videos in a list\n",
    "list_relevant_data = []\n",
    "\n",
    "# Reading the file\n",
    "with gzip.open('/dlabdata1/youtube_large/yt_metadata_en_dd.jsonl.gz', 'rb') as f:\n",
    "\n",
    "    for i, line in enumerate(f):\n",
    "        if i % 1000000 == 0 and i != 0:\n",
    "        print('Progress: ' + str(int(idx/1000000)) + '/85')\n",
    "\n",
    "        if i in set_relevant_vid:\n",
    "\n",
    "            # line is a str dict, video is the dict corresponding to the str dict\n",
    "            video = json.loads(line)\n",
    "\n",
    "            list_vid_relevant_features = [video['channel_id']]\n",
    "            list_vid_relevant_features.append(video['view_count'])\n",
    "            list_vid_relevant_features.append(video['upload_date'][:4])\n",
    "            list_vid_relevant_features.append(video['categories'])\n",
    "\n",
    "            list_relevant_data.append(list_vid_relevant_features)\n",
    "\n",
    "            idx_new += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevant_data = pd.DataFrame(list_relevant_data, columns=columns_names)\n",
    "df_relevant_data_top20 = df_relevant_data.sort_values(['view_counts'], ascending=False).groupby(\n",
    "    ['category', 'uploaded_year', 'channel_id']).head(20)\n",
    "\n",
    "set_relevant_vid_top20 = sorted(df_relevant_data_top20.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate result\n",
    "with open('/dlabdata1/youtube_large/olam/data/final_res/model/set_relevant_vid_top20.pickle', 'wb') as f:\n",
    "    pickle.dump(sorted_idx_relevant_vid_top20, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only relevant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate data\n",
    "S = scipy.sparse.load_npz(\n",
    "    '/dlabdata1/youtube_large/olam/data/final_res/matrices/S_full.npz')\n",
    "\n",
    "\n",
    "# Load set of videos in the top20 as processed abose\n",
    "with open('/dlabdata1/youtube_large/olam/data/final_res/model/set_relevant_vid_top20.pickle', 'rb') as f:\n",
    "    set_relevant_vid_top20 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = S[set_relevant_vid_top20, :]\n",
    "\n",
    "# Convert matrix to csc for efficient computing\n",
    "S = S.tocsc()\n",
    "\n",
    "list_relevant_tokens = []\n",
    "\n",
    "# Iterate on the columns\n",
    "for i in range(S_full.shape[1]):\n",
    "\n",
    "    if i % 1000000 == 0:\n",
    "        print('Processed : ' + str(i) + ' tokens')\n",
    "\n",
    "    # Check column has more than 100 non zero entries\n",
    "    if S_full[:, i].count_nonzero() >= 100:\n",
    "        list_relevant_tokens.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = S[:, list_relevant_tokens]\n",
    "S = S.tocsr()\n",
    "scipy.sparse.save_npz('/dlabdata1/youtube_large/olam/data/final_res/matrices/S_final.npz', S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dictionnary id2word for final sparse matrix\n",
    "\n",
    "`id2word_top20` is a dictionnary that map each of the token id to the token itself only for relevant tokens for top20 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_top20 = {}\n",
    "\n",
    "for i, id_token in enumerate(list_relevant_tokens):\n",
    "    id2word_top20[i] = id2word[id_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Spark dataframe to use the hadoop cluster to perform topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[4]\").setAll(\n",
    "    [('spark.executor.memory', '4g'), ('spark.driver.memory', '16g'), ('spark.driver.maxResultSize', '0')])\n",
    "\n",
    "# create the session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_for_row(row, S):\n",
    "    '''Construct SparseVector bag-of-word for each row (videos)'''\n",
    "    tmp_dict = {}\n",
    "    for key, value in row:\n",
    "        tmp_dict[key[1]] = value\n",
    "\n",
    "    return SparseVector(S.shape[1], tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "print('Process video for topic modelling...')\n",
    "for i in range(S.shape[0]):\n",
    "\n",
    "    if i % 1000000 == 0:\n",
    "        print(str(i) + ' videos processed...')\n",
    "\n",
    "    data.append([i, get_dict_for_row(S.getrow(i).todok().items(), S)])\n",
    "\n",
    "\n",
    "# Construct dataframe for LDA\n",
    "all_df = spark.createDataFrame(data, [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "all_df.write.option('compression', 'gzip').json(\n",
    "    '/dlabdata1/youtube_large/olam/data/final_res/model/sparkdf.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Topic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakedGensimDict:\n",
    "    \"\"\"\n",
    "    Locally made class for `~gensim.corpora.dictionary.Dictionary`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, S):\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError('`data` must be an instance of `dict`')\n",
    "\n",
    "        self.id2token = data\n",
    "        self.token2id = {v: k for k, v in data.items()}\n",
    "        self.doc2bow = S\n",
    "\n",
    "    @staticmethod\n",
    "    def from_vocab(vocab):\n",
    "        return FakedGensimDict(dict(zip(range(len(vocab)), vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the coherence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "for i in range(S.shape[0]):\n",
    "    token_indices = list(S.getrow(i).nonzero()[1])\n",
    "    tokens = []\n",
    "\n",
    "    for token_indice in token_indices:\n",
    "        tokens.append(id2word_top20[token_indice])\n",
    "    texts.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for row in S.toarray():\n",
    "    bow = []\n",
    "    idx_nonzero = np.nonzero(row)[0]\n",
    "    for i in range(len(idx_nonzero)):\n",
    "        bow.append((idx_nonzero[i], row[idx_nonzero[i]]))\n",
    "    corpus.append(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the coherence scores for each model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_scores = []\n",
    "coherence_scores_umass = []\n",
    "\n",
    "n_topics_list = [40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95,\n",
    "                 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160]\n",
    "\n",
    "for i, n_topics in enumerate(n_topics_list):\n",
    "\n",
    "    print('Computing coherence score for model with ' +\n",
    "          str(n_topics) + ' topics...')\n",
    "\n",
    "    # Get describe_topics dataframe\n",
    "    describe_topics = spark.read.json(\n",
    "        '/dlabdata1/youtube_large/olam/data/final_res/model/tune/describe_topics_' + str(n_topics) + '.json')\n",
    "\n",
    "    # Characterize the topics with tokens\n",
    "    topics = []\n",
    "\n",
    "    for row in describe_topics.sort('topic').rdd.collect():\n",
    "        tokenized_topic = []\n",
    "        for j, token_id in enumerate(row.termIndices):\n",
    "            tokenized_topic.append(id2word_top20[token_id])\n",
    "            if j > 10:\n",
    "                break\n",
    "        topics.append(tokenized_topic)\n",
    "\n",
    "    # Compute c_v coherence score and append to coherence scores\n",
    "    coherence_model = CoherenceModel(topics=topics,\n",
    "                                     corpus=S,\n",
    "                                     dictionary=FakedGensimDict(\n",
    "                                         id2word_top20, S),\n",
    "                                     texts=texts,\n",
    "                                     coherence='c_v')\n",
    "\n",
    "    # Compute u_mass coherence score and append to coherence scores\n",
    "    coherence_model_umass = CoherenceModel(topics=topics,\n",
    "                                           corpus=corpus,\n",
    "                                           dictionary=FakedGensimDict(\n",
    "                                               id2word_top20, S),\n",
    "                                           coherence='u_mass')\n",
    "\n",
    "    print('Getting c_v coherence score...')\n",
    "    coherence_scores.append(coherence_model.get_coherence())\n",
    "    print('Getting u_mass coherence score...')\n",
    "    coherence_scores_umass.append(coherence_model_umass.get_coherence())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "ax.set_title('Coherence score for a given number of topics', fontsize=24)\n",
    "ax.set_xlabel('Number of Topics', fontsize=16)\n",
    "ax.set_ylabel('Coherence Score c_v', fontsize=16)\n",
    "\n",
    "ax.grid('on')\n",
    "\n",
    "ax.plot(coherence_scores, label='c_v coherence score', linewidth=3)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel('Coherence Score u_mass', fontsize=16)\n",
    "ax2.plot(coherence_scores_umass, label='u_mass coherence score', linewidth=3, color='orange')\n",
    "\n",
    "ax.legend(fontsize=16)\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1, 0.93), fontsize=16)\n",
    "\n",
    "plt.xticks(np.arange(len(n_topics_list)), n_topics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('/home/olam/coherence_scores')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "465.8088073730469px",
    "left": "1108.0882568359375px",
    "top": "289.4668884277344px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
