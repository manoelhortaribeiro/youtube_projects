{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import random\n",
    "import scipy.sparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import LDA, LDAModel, LocalLDAModel\n",
    "from pyspark.ml.linalg import Vectors, SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[12]\").setAll([\n",
    "                                   ('spark.executor.memory', '16g'),  # find\n",
    "                                   ('spark.driver.memory','8g'), # your\n",
    "                                   ('spark.driver.maxResultSize', '4G') # setup\n",
    "                                  ])\n",
    "# create the session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# create the context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process train_data and test_data\n",
    "\n",
    "We tune the optimal number of topics by minimizing the perplexity on subsample of videos, randomly taken from all the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = scipy.sparse.load_npz('/dlabdata1/youtube_large/olam/matrices/S_final2.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to tune the optimal number of topics => find it on random subset of videos\n",
    "id_vid2train = random.sample(range(0,S.shape[0]), 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_sub = S[id_vid2train,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100000x663127 sparse matrix of type '<class 'numpy.uint8'>'\n",
       "\twith 1823560 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_for_row(row):\n",
    "    ''''''\n",
    "    tmp_dict = {}\n",
    "    for key, value in row:\n",
    "        tmp_dict[key[1]] = value\n",
    "\n",
    "    return SparseVector(S_sub.shape[1], tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_idx = set(random.sample(range(0,S_sub.shape[0]), int(0.85*S_sub.shape[0])))\n",
    "len(train_data_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 videos processed\n"
     ]
    }
   ],
   "source": [
    "train_data_idx_sorted = 0\n",
    "test_data_idx_sorted = 0\n",
    "\n",
    "for i in range(S_train.shape[0]):\n",
    "    \n",
    "    if i % 10000000 == 0:\n",
    "        print(str(i) + ' videos processed')\n",
    "    \n",
    "    # Data is a list of list of the following elems : index of doc and a bag-of-word sparse Vector\n",
    "    if i in train_data_idx:\n",
    "        train_data.append([train_data_idx_sorted, get_dict_for_row(S_train.getrow(i).todok().items())])\n",
    "        train_data_idx_sorted += 1\n",
    "    else:\n",
    "        test_data.append([test_data_idx_sorted, get_dict_for_row(S_train.getrow(i).todok().items())])\n",
    "        test_data_idx_sorted += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the best number of topics\n",
    "\n",
    "Since we are selecting only the best number of topics from subsamples of videos, no need to keep index of the videos\n",
    "\n",
    "- train the model on `train_data`\n",
    "compute log-perplexity on `test_data`\n",
    "\n",
    "- choose the number of topics such that log-perplex is minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_topics = [6, 8, 10, 12]\n",
    "perplex_scores = []\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.createDataFrame(train_data, [\"id\", \"features\"])\n",
    "test_df = spark.createDataFrame(test_data, [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing with 6 topics...\n",
      "Computing with 8 topics...\n",
      "Computing with 10 topics...\n",
      "Computing with 12 topics...\n",
      "The optimal choice for the number of topics : 6\n"
     ]
    }
   ],
   "source": [
    "for n_topic in numbers_topics:\n",
    "    print('Computing with ' + str(n_topic) + ' topics...')\n",
    "    lda = LDA(k=n_topic, seed=1)\n",
    "    model = lda.fit(train_df)\n",
    "    logperplexity = model.logPerplexity(test_df)\n",
    "    \n",
    "    models.append(model)\n",
    "    perplex_scores.append(logperplexity)\n",
    "    \n",
    "    \n",
    "    \n",
    "print('The optimal choice for the number of topics : ' + str(numbers_topics[np.argmin(perplex_scores)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.257216790716088, 18.78972936215072, 23.02752573719546, 27.81120252103568]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplex_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_opt = numbers_topics[np.argmin(perplex_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_columns = []\n",
    "for i in range(n_topics_opt):\n",
    "    topic_columns.append('Topic' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(S.shape[0]):\n",
    "    \n",
    "    if i % 10000000 == 0:\n",
    "        print(str(i) + ' videos processed')\n",
    "    \n",
    "    # Data is a list of list of the following elems : index of doc and a bag-of-word sparse Vector\n",
    "    data.append([i, get_dict_for_row(S_train.getrow(i).todok().items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=n_topics_opt, seed=1)\\\n",
    "                .setTopicDistributionCol('topicDistributionCol')\\\n",
    "                .setK(n_topics_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lda.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model with its attributes for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/dlabdata1/youtube_large/olam/LDA_Model/pyspark_ldamodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attributes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attributes['perplexity'] = model.logPerplexity(df)\n",
    "model_attributes['vocabSize'] = model.vocabSize()\n",
    "model_attributes['n_topic'] = model.getK()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perplexity': 10.335341077216416, 'vocabSize': 663127, 'n_topic': 6}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.describeTopics(maxTermsPerTopic=10).write\\\n",
    "                    .option('compression', 'gzip')\\\n",
    "                    .json('/dlabdata1/youtube_large/olam/LDA_Model/describe_topics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(model.topicsMatrix().toArray().tolist(), topic_columns)\\\n",
    "                    .write\\\n",
    "                    .option('compression', 'gzip')\\\n",
    "                    .json('/dlabdata1/youtube_large/olam/LDA_Model/topics_term_matrix.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(train_df).write\\\n",
    "                    .option('compression', 'gzip')\\\n",
    "                    .json('/dlabdata1/youtube_large/olam/LDA_Model/topics_doc_matrix.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/dlabdata1/youtube_large/olam/LDA_Model/lda_model_attributes.pickle', 'wb') as f:\n",
    "    pickle.dump(model_attributes, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = LocalLDAModel.load('/dlabdata1/youtube_large/olam/LDA_Model/pyspark_ldamodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.isDistributed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionnary of tokens\n",
    "with open('/dlabdata1/youtube_large/olam/LDA_Model/lda_model_attributes.pickle', 'rb') as f:\n",
    "    loaded_model_attributes = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perplexity': 10.335341077216416, 'vocabSize': 663127, 'n_topic': 6}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.json('/dlabdata1/youtube_large/olam/LDA_Model/describe_topics.json')\n",
    "df2 = spark.read.json('/dlabdata1/youtube_large/olam/LDA_Model/topics_term_matrix.json')\n",
    "df3 = spark.read.json('/dlabdata1/youtube_large/olam/LDA_Model/topics_doc_matrix.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[termIndices: array<bigint>, termWeights: array<double>, topic: bigint]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Topic0: double, Topic1: double, Topic2: double, Topic3: double, Topic4: double, Topic5: double]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: struct<indices:array<bigint>,size:bigint,type:bigint,values:array<double>>, id: bigint, topicDistributionCol: struct<type:bigint,values:array<double>>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand results from LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Get tokens of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionnary of tokens\n",
    "with open('/dlabdata1/youtube_large/olam/id2word_2.pickle', 'rb') as f:\n",
    "    id2word = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_decribe = models[0].describeTopics(maxTermsPerTopic=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[146119, 25193, 3...|[0.00155618833174...|\n",
      "|    1|[598533, 69659, 6...|[0.00296299689040...|\n",
      "|    2|[60788, 655263, 3...|[0.00601695425345...|\n",
      "|    3|[118607, 627194, ...|[0.00109346516697...|\n",
      "|    4|[561976, 39325, 6...|[0.00105658456549...|\n",
      "|    5|[658082, 347101, ...|[0.00125926833773...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_decribe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "   With weight of 0.0015561883317400421 : beat\n",
      "   With weight of 0.0010150868096907332 : type\n",
      "   With weight of 0.00043026050299143693 : monkey\n",
      "   With weight of 0.00038950086709461386 : instrument\n",
      "   With weight of 0.00032512815329500895 : galaxi\n",
      "Topic 1: \n",
      "   With weight of 0.002962996890404119 : music\n",
      "   With weight of 0.0007333861519990407 : song\n",
      "   With weight of 0.0005652978340863234 : guitar\n",
      "   With weight of 0.0004963401621737965 : new\n",
      "   With weight of 0.0004889577751426235 : video\n",
      "Topic 2: \n",
      "   With weight of 0.006016954253455886 : news\n",
      "   With weight of 0.005686152362795667 : game\n",
      "   With weight of 0.004070489911662575 : video\n",
      "   With weight of 0.003236206563313916 : movi\n",
      "   With weight of 0.002747477712124675 : gameplay\n",
      "Topic 3: \n",
      "   With weight of 0.0010934651669757037 : pokemon\n",
      "   With weight of 0.0006376327416773075 : fortnit\n",
      "   With weight of 0.0005805983087450661 : asmr\n",
      "   With weight of 0.00033741058447619727 : world\n",
      "   With weight of 0.0003008724356315609 : nail\n",
      "Topic 4: \n",
      "   With weight of 0.001056584565492997 : nba\n",
      "   With weight of 0.0006995668283457584 : roblox\n",
      "   With weight of 0.00038204465544335395 : game\n",
      "   With weight of 0.0002884960783961685 : slime\n",
      "   With weight of 0.00026727880892274506 : train\n",
      "Topic 5: \n",
      "   With weight of 0.001259268337737569 : show\n",
      "   With weight of 0.0011746560716629634 : video\n",
      "   With weight of 0.001088691193314929 : news\n",
      "   With weight of 0.0009936282628427982 : review\n",
      "   With weight of 0.0008901431488903228 : live\n"
     ]
    }
   ],
   "source": [
    "for row in topics_decribe.rdd.collect():\n",
    "    print('Topic ' + str(row.topic) + ': ')\n",
    "    term_weights = row.termWeights\n",
    "    for i, token_id in enumerate(row.termIndices):\n",
    "        print('   With weight of ' + str(term_weights[i]) + ' : ' + id2word[token_id] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Get visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicTermDist = model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(663127, 20, [0.9083, 0.7525, 0.8744, 0.7795, 0.7803, 0.8172, 0.7363, 0.8225, ..., 0.8475, 0.7457, 0.5989, 0.9623, 0.588, 0.7651, 0.7585, 0.8564], 0)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicTermDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue : need to remove tokens of length 1/2 , tokens with another alphabet and numerical tokens\n",
    "\n",
    "Hence, save the S_final2 sparse matrix and get the id2word2 dict that map the token_id to the token in the new matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_token_to_remove = []\n",
    "token_to_keep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def englishAlpha(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_id, token in id2word.items():\n",
    "    if len(token) < 3 or nonEnglishAlpha(token) or token.isnumeric():\n",
    "        id_token_to_remove.append(token_id)\n",
    "    else:\n",
    "        token_to_keep.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81014"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_token_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663127"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_to_keep) + len(id_token_to_remove) == len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<68638982x744141 sparse matrix of type '<class 'numpy.uint8'>'\n",
       "\twith 1393937498 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_token_to_keep = (np.delete(np.arange(len(id2word)), id_token_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = S[:, id_token_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<68638982x663127 sparse matrix of type '<class 'numpy.uint8'>'\n",
       "\twith 1251231562 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_keep = set(token_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_new = {}\n",
    "\n",
    "k = 0\n",
    "for i, token in enumerate(id2word.values()):\n",
    "    if token in token_to_keep:\n",
    "        id2word_new[k] = token\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('/dlabdata1/youtube_large/olam/matrices/S_final2.npz', S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/dlabdata1/youtube_large/olam/id2word_2.pickle', 'wb') as f:\n",
    "    pickle.dump(id2word_new, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
