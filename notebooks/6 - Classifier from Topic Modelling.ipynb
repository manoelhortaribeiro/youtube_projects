{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/olam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import json\n",
    "import nltk\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import random\n",
    "import scipy.sparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select select the data\n",
    "\n",
    "### Selection criteria\n",
    "- Not used to train the topic modelling\n",
    "- Video with more than 10'000 views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        '''Init method'''\n",
    "        self.fh = open(file,'rb')\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstd.ZstdDecompressor()\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "\n",
    "    def readlines(self):\n",
    "        '''Generator method that creates an iterator for each line of JSON'''\n",
    "        while True:\n",
    "            chunk = self.reader.read(self.chunk_size).decode(errors=\"ignore\")\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load set of videos to consider\n",
    "with open('/dlabdata1/youtube_large/olam/data/view10000_sub10000/idx_vid_to_consider.pickle', 'rb') as f:\n",
    "    idx_vid_to_consider = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load channels\n",
    "df_channelcrawler = pd.read_csv('/dlabdata1/youtube_large/channelcrawler.csv')\n",
    "\n",
    "df_channelcrawler['channel_id'] = df_channelcrawler['link'].apply(lambda x: x.replace('http://www.youtube.com/channel/', ''))\n",
    "\n",
    "# filter the channels\n",
    "df_channelcrawler_100000sub = df_channelcrawler[df_channelcrawler['subscribers'] >= 100000]\n",
    "set_channelcrawler_100000sub = set(df_channelcrawler_100000sub['channel_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1/85\n",
      "Progress: 2/85\n",
      "Progress: 3/85\n",
      "Progress: 4/85\n",
      "Progress: 5/85\n",
      "Progress: 6/85\n",
      "Progress: 7/85\n",
      "Progress: 8/85\n",
      "Progress: 9/85\n",
      "Progress: 10/85\n",
      "Progress: 11/85\n",
      "Progress: 12/85\n",
      "Progress: 13/85\n",
      "Progress: 14/85\n",
      "Progress: 15/85\n",
      "Progress: 16/85\n",
      "Progress: 17/85\n",
      "Progress: 18/85\n",
      "Progress: 19/85\n",
      "Progress: 20/85\n",
      "Progress: 21/85\n",
      "Progress: 22/85\n",
      "Progress: 23/85\n",
      "Progress: 24/85\n",
      "Progress: 25/85\n",
      "Progress: 26/85\n",
      "Progress: 33/85\n",
      "Progress: 34/85\n",
      "Progress: 35/85\n",
      "Progress: 36/85\n",
      "Progress: 37/85\n",
      "Progress: 38/85\n",
      "Progress: 39/85\n",
      "Progress: 40/85\n",
      "Progress: 41/85\n",
      "Progress: 42/85\n",
      "Progress: 43/85\n",
      "Progress: 44/85\n",
      "Progress: 45/85\n",
      "Progress: 46/85\n",
      "Progress: 47/85\n",
      "Progress: 48/85\n",
      "Progress: 49/85\n",
      "Progress: 50/85\n",
      "Progress: 51/85\n",
      "Progress: 52/85\n",
      "Progress: 53/85\n",
      "Progress: 54/85\n",
      "Progress: 55/85\n",
      "Progress: 56/85\n",
      "Progress: 57/85\n",
      "Progress: 58/85\n",
      "Progress: 59/85\n",
      "Progress: 60/85\n",
      "Progress: 61/85\n",
      "Progress: 62/85\n",
      "Progress: 63/85\n",
      "Progress: 64/85\n",
      "Progress: 65/85\n",
      "Progress: 66/85\n",
      "Progress: 67/85\n",
      "Progress: 68/85\n",
      "Progress: 69/85\n",
      "Progress: 70/85\n",
      "Progress: 71/85\n",
      "Progress: 72/85\n",
      "Progress: 73/85\n",
      "Progress: 74/85\n",
      "Progress: 75/85\n",
      "Progress: 76/85\n",
      "Progress: 77/85\n",
      "Progress: 78/85\n",
      "Progress: 79/85\n",
      "Progress: 80/85\n",
      "Progress: 81/85\n",
      "Progress: 82/85\n",
      "Progress: 83/85\n",
      "Progress: 84/85\n",
      "Progress: 85/85\n"
     ]
    }
   ],
   "source": [
    "reader = Zreader(\"/dlabdata1/youtube_large/yt_metadata_all.jsonl.zst\", chunk_size=2**28)\n",
    "\n",
    "idx = 0\n",
    "array_relevant_infos = []\n",
    "\n",
    "for line in reader.readlines():\n",
    "    ###start_iter = time.time()\n",
    "    idx += 1\n",
    "    \n",
    "    if idx % 1000000 == 0:\n",
    "        print('Progress: ' + str(int(idx/1000000)) + '/85')\n",
    "        \n",
    "    if idx in idx_vid_to_consider:\n",
    "        \n",
    "        # line is a str dict, video is the dict corresponding to the str dict\n",
    "        video = json.loads(line)\n",
    "        \n",
    "        array_vid_relevant_infos = [video['channel_id']]\n",
    "        array_vid_relevant_infos.append(video['view_count'])\n",
    "        array_vid_relevant_infos.append(video['upload_date'][:4])\n",
    "        array_vid_relevant_infos.append(video['categories'])\n",
    "        \n",
    "        array_relevant_infos.append(array_vid_relevant_infos)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe of all the videos that we will consider\n",
    "\n",
    "column_names = ['channel_id', 'view_counts', 'uploaded_year', 'category']\n",
    "\n",
    "df = pd.DataFrame(array_relevant_infos, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21714294, 4)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all the videos that are used for topic modelling\n",
    "\n",
    "Find the video indices in the dataframe such that:\n",
    "- more than 100'000 subscribers\n",
    "- top20 from category/channel/year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>view_counts</th>\n",
       "      <th>uploaded_year</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21326653</th>\n",
       "      <td>UC0C-w0YjGpqDXGB8IHb662A</td>\n",
       "      <td>4468090305</td>\n",
       "      <td>2017</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648298</th>\n",
       "      <td>UCVp3nfGRxmMadNDuVbJSk8A</td>\n",
       "      <td>4295905423</td>\n",
       "      <td>2015</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7977749</th>\n",
       "      <td>UCcdwLMPsaU2ezNSJU1nFoBQ</td>\n",
       "      <td>3838039119</td>\n",
       "      <td>2016</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4655616</th>\n",
       "      <td>UCmfFGTSsfJVu6CGvL8r75qg</td>\n",
       "      <td>3709532958</td>\n",
       "      <td>2014</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13575776</th>\n",
       "      <td>UCN1hnUccO4FD5WfM7ithXaw</td>\n",
       "      <td>3055180938</td>\n",
       "      <td>2015</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        channel_id  view_counts uploaded_year   category\n",
       "21326653  UC0C-w0YjGpqDXGB8IHb662A   4468090305          2017      Music\n",
       "10648298  UCVp3nfGRxmMadNDuVbJSk8A   4295905423          2015      Music\n",
       "7977749   UCcdwLMPsaU2ezNSJU1nFoBQ   3838039119          2016  Education\n",
       "4655616   UCmfFGTSsfJVu6CGvL8r75qg   3709532958          2014      Music\n",
       "13575776  UCN1hnUccO4FD5WfM7ithXaw   3055180938          2015      Music"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub100000 = df[df['channel_id'].isin(set_channelcrawler_100000sub)]\n",
    "df_top20 = df_sub100000.sort_values(['view_counts'], ascending=False).groupby(['category', 'uploaded_year', 'channel_id']).head(20)\n",
    "df_top20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove = set(df_top20.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_data = []\n",
    "\n",
    "for index in df.index:\n",
    "    if index not in index_to_remove:\n",
    "        index_data.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17801854"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/dlabdata1/youtube_large/olam/data/classifier/index_data.pickle', 'wb') as f:\n",
    "    pickle.dump(index_data, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the data\n",
    "\n",
    "- Transform every video into BoW, according to the topic model vocabulary\n",
    "- Get the transformed data -> distribution over the topic for each video\n",
    "- Separate into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglishAlpha(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_tokens_per_video(video):\n",
    "    ''''''\n",
    "    \n",
    "    title_tokens = [w for w in tokenizer.tokenize(video['title'].lower()) if not w in stop_words]\n",
    "    tag_tokens = [w for w in tokenizer.tokenize(video['tags'].lower()) if not w in stop_words]\n",
    "    \n",
    "    # We want to keep duplicates !!\n",
    "    tokens_per_video = title_tokens + tag_tokens\n",
    "\n",
    "    # Filter token with length < 3, with non english alphabet since fastext is not 100% accurate and remove numerical token \n",
    "    tokens_keep = []\n",
    "    for token in tokens_per_video:\n",
    "        if len(token) >= 3 and (not token.isnumeric()) and isEnglishAlpha(token):\n",
    "            tokens_keep.append(token)\n",
    "    \n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_tokens_per_video = ([s_stemmer.stem(w) for w in tokens_keep])\n",
    "    \n",
    "    \n",
    "    # Return a Counter object of the tokens\n",
    "    return collections.Counter(tokens_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_underlying_dict(freq_tokens_per_video, dict_stemmed_tokens, dict_freq_tokens_for_sparse_matrix, i_vid):\n",
    "    '''Method to fill the underlying dictionnary in order to \n",
    "    update the sparse matrix incrementally by videos'''\n",
    "    \n",
    "    for key in freq_tokens_per_video.keys():\n",
    "        \n",
    "        # Column index in the sparse matrix (one column for each token)\n",
    "        j_token = dict_stemmed_tokens[key]\n",
    "    \n",
    "        # Filling the underlying dict\n",
    "        dict_freq_tokens_for_sparse_matrix[(i_vid % 1000000, j_token)] = freq_tokens_per_video[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionnary of words\n",
    "with open('/dlabdata1/youtube_large/olam/data/view10000_sub100000/id2word_tok100vid_sub100000.pickle', 'rb') as f:\n",
    "    id2word = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Load index of data for classifier\n",
    "with open('/dlabdata1/youtube_large/olam/data/classifier/index_data.pickle', 'rb') as f:\n",
    "    index_data = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(id2word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dok_matrix((1000000, len(vocab)), dtype=np.uint8)\n",
    "groundtruth = []\n",
    "\n",
    "\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/yt_metadata_all.jsonl.zst\", chunk_size=2**28)\n",
    "\n",
    "idx = 0\n",
    "i_vid = 0\n",
    "\n",
    "for line in reader.readlines():\n",
    "    ###start_iter = time.time()\n",
    "    idx += 1\n",
    "    \n",
    "    if idx == 52:\n",
    "        break\n",
    "        \n",
    "    if i_vid % 1000000 == 0 and i_vid != 0:\n",
    "        print('Size of matrix dok: ' + str(sys.getsizeof(data)))\n",
    "        print('Shape of S : ' + str(data.get_shape()) + ' and number of elems : ' + str(data.getnnz()))\n",
    "        data = data.tocsr()\n",
    "        print('Size of matrix csr: ' + str(sys.getsizeof(data)))\n",
    "        file_name = 'data' + str(int(idx / 1000000))\n",
    "        scipy.sparse.save_npz('/dlabdata1/youtube_large/olam/data/classifier/csr_matrices/' + file_name + '.npz', data)\n",
    "        data = dok_matrix((1000000, len(vocab)), dtype=np.uint8)\n",
    "        print('Shape of S : ' + str(data.get_shape()) + ' and number of elems : ' + str(data.getnnz()))\n",
    "        print('Processed ' + str(i_vid) + ' videos.')\n",
    "        print('')\n",
    "        \n",
    "    if idx in index_data:\n",
    "        \n",
    "        # line is a str dict, video is the dict corresponding to the str dict\n",
    "        video = json.loads(line)\n",
    "        \n",
    "        # For each video, create a underlying dictionnary for filling the sparse matrix efficiently\n",
    "        dict_freq_tokens_for_sparse_matrix = {}\n",
    "        \n",
    "        # Get the tokens for each video and theirs number of occurences\n",
    "        freq_tokens_per_video = get_freq_tokens_per_video(video)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categories': 'Howto & Style',\n",
       " 'channel_id': 'UCzzzZ3-icktxbC3j7hkWqRw',\n",
       " 'crawl_date': '2019-11-08 05:24:08.486341',\n",
       " 'description': \"At Citalia, we love Sicily and so does Gennaro Contaldo. In this video he shares his thoughts on the history, landscape, beaches and towns of this wonderful island.\\n\\nTo learn more about Sicliy, visit http://www.citalia.com/holidays/italy/sicily-and-the-aeolian-islands/\\n\\nAs he says, in Sicily, there's always something to discover.\\n\\nSicily is the largest and most populated island in the Mediterranean. Lying close to the mainland of Italy and just 90 miles from the African coastline, it retains Arab as well as Greek and Roman influences in its architecture and culture. Sicily is dominated by the dramatic landscapes around Mount Etna, Europe's most active volcano and the highest mountain in Italy south of the Alps. An expedition to see it, or even climb it, is an essential part of any visit - and a sight to thrill the whole family.\\n\\nSicily's towns and cities are each unique. Palermo is a chaotic mix of crumbling villas, ancient churches and buzzing markets selling everything from seafood and fresh mozzarella, to vintage clothes and antiques, and where the atmosphere owes much to the souks of North Africa. Cefalù is a bustling seaside resort with a wide sandy beach and meandering cobbled streets with shops selling local crafts. Many visitors, however, spend their time in Taormina and it's easy to see why. It's often called the most beautiful town in Sicily and makes the most of its natural setting that takes in views of Mount Etna and the sparkling Mediterranean.\",\n",
       " 'dislike_count': 3,\n",
       " 'display_id': 'rY3VfvMuUNw',\n",
       " 'duration': 216,\n",
       " 'like_count': 124,\n",
       " 'tags': 'Sicily,Italy,Italian,Travel,Tips,Gennaro Contaldo,Holiday,Vacation,Toarmina,Palermo,Syracuse,beaches,Trapani,Sicilian coast,churches,Cefalù,cathedrals,mountains,hiking,skiing,harbors,fishing,caves,lakes,volcanoes,theatre,climbing,cliffs,kitesurfing,waterfalls,music,palaces,wine,golf,beer,tennis,castles',\n",
       " 'title': 'Sicily Travel Tips from Gennaro Contaldo | Citalia',\n",
       " 'upload_date': '2016-06-27 00:00:00',\n",
       " 'view_count': 7377}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the classifier\n",
    "\n",
    "- Train set into train' and validation set, in order to do cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
